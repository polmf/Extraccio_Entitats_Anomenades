{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pycrfsuite\n",
    "from nltk.corpus import conll2002\n",
    "from nltk.tag import CRFTagger\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # Tokenitzador\n",
    "nltk.download('averaged_perceptron_tagger') # Etiquetador POS\n",
    "nltk.download('maxent_ne_chunker') # Etiquetador Entitats Anomenades\n",
    "nltk.download('words')\n",
    "nltk.download('treebank')\n",
    "nltk.download('conll2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_esp = conll2002.iob_sents('esp.train') # Train\n",
    "testa_esp = conll2002.iob_sents('esp.testa') # Dev\n",
    "testb_esp = conll2002.iob_sents('esp.testb') # Test\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "testa_ned = conll2002.iob_sents('ned.testa') # Dev\n",
    "testb_ned = conll2002.iob_sents('ned.testb') # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tagger_POS = CRFTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, use_basic_features=False, use_prefix_suffix_features=False, use_context_features=False, pattern = r'\\d+'):\n",
    "        self.use_basic_features = use_basic_features\n",
    "        self.use_prefix_suffix_features = use_prefix_suffix_features\n",
    "        self.use_context_features = use_context_features\n",
    "        self._pattern = pattern\n",
    "        \n",
    "    def _get_features(self, tokens, idx):\n",
    "        \"\"\"\n",
    "        Extract basic features about this word including\n",
    "            - Current word\n",
    "            - is it capitalized?\n",
    "            - Does it have punctuation?\n",
    "            - Does it have a number?\n",
    "            - Preffixes up to length 3\n",
    "            - Suffixes up to length 3\n",
    "            - paraules prèvies i posteriors amb POS\n",
    "            - POS-tags\n",
    "            - longitud\n",
    "\n",
    "        Note that : we might include feature over previous word, next word etc.\n",
    "\n",
    "        :return: a list which contains the features\n",
    "        :rtype: list(str)\n",
    "        \"\"\"\n",
    "            \n",
    "        token = tokens[idx]\n",
    "        \n",
    "        feature_list = []\n",
    "        \n",
    "        if self.use_basic_features:\n",
    "            # Capitalization\n",
    "            if token[0].isupper():\n",
    "                feature_list.append(\"CAPITALIZATION\")\n",
    "\n",
    "            # Number\n",
    "            if re.search(self._pattern, token) is not None:\n",
    "                feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "            # Punctuation\n",
    "            punc_cat = {\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"}\n",
    "            if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "                feature_list.append(\"PUNCTUATION\")\n",
    "                    \n",
    "        if self.use_prefix_suffix_features:\n",
    "            # preffix up to length 3\n",
    "            if len(token) > 1:\n",
    "                feature_list.append(\"PRE_\" + token[:1])\n",
    "            if len(token) > 2:\n",
    "                feature_list.append(\"PRE_\" + token[:2])\n",
    "            if len(token) > 3:\n",
    "                feature_list.append(\"PRE_\" + token[:3])\n",
    "\n",
    "            # Suffix up to length 3\n",
    "            if len(token) > 1:\n",
    "                feature_list.append(\"SUF_\" + token[-1:])\n",
    "            if len(token) > 2:\n",
    "                feature_list.append(\"SUF_\" + token[-2:])\n",
    "            if len(token) > 3:\n",
    "                feature_list.append(\"SUF_\" + token[-3:])\n",
    "    \n",
    "        \n",
    "        if self.use_context_features:\n",
    "            # POS_tags\n",
    "            POS = model_tagger_POS.tag(tokens)\n",
    "                \n",
    "            # Paraules prèvies amb POS\n",
    "            if idx > 0:\n",
    "                feature_list.append(\"anterior1_\" + tokens[idx-1] + \"_\" + POS[idx-1][1])\n",
    "            if idx > 1:\n",
    "                feature_list.append(\"anterior2_\" + tokens[idx-2] + \"_\" + POS[idx-2][1])\n",
    "                \n",
    "            # Paraules posteriors amb POS\n",
    "            if idx < (len(tokens)-1):\n",
    "                feature_list.append(\"posterior1_\" + tokens[idx+1] + \"_\" + POS[idx+1][1])\n",
    "            if idx < (len(tokens)-2):\n",
    "                feature_list.append(\"posterior2_\" + tokens[idx+2] + \"_\" + POS[idx+2][1])\n",
    "\n",
    "            feature_list.append(\"WORD_\" + token)\n",
    "            \n",
    "        \n",
    "        return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_token(fitxer):\n",
    "    res = []\n",
    "    for sentence in fitxer:\n",
    "        frases = []\n",
    "        for elem1, elem2, elem3 in sentence:\n",
    "            frases.append(elem1)\n",
    "        res.append(frases)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicció amb BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def obtener_token_POS(fitxer):\n",
    "    res = []\n",
    "    for sentence in fitxer:\n",
    "        frases = []\n",
    "        for elem1, elem2, elem3 in sentence:\n",
    "            frases.append((elem1, elem2))\n",
    "        res.append(frases)\n",
    "    return res\"\"\"\n",
    "\n",
    "def obtener_token_BIO(fitxer):\n",
    "    res = []\n",
    "    for sentence in fitxer:\n",
    "        frases = []\n",
    "        for elem1, elem2, elem3 in sentence:\n",
    "            frases.append((elem1, elem3))\n",
    "        res.append(frases)\n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaluació amb sets veure si son iguals també es pot mirar per intersecció i si coincideixen\n",
    "def obtener_entidades_con_posiciones_prova(testa_esp_BIO_tag):\n",
    "    entitats_con_posiciones = set()\n",
    "\n",
    "    for sentence_index, sentence in enumerate(testa_esp_BIO_tag):\n",
    "        ent = []\n",
    "        name = None\n",
    "        start_pos = None  # Posición de inicio de la entidad actual\n",
    "        prev_tag = None  # Almacenar la etiqueta del token anterior\n",
    "\n",
    "        for token_index, token in enumerate(sentence):\n",
    "            word, tag = token\n",
    "            #word = word[0]\n",
    "\n",
    "            if tag.startswith('B-'):\n",
    "                # Si hay una entidad anterior, la agregamos a la lista de entidades\n",
    "                if ent:\n",
    "                    end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                    entitats_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Creamos una nueva entidad con la palabra actual\n",
    "                ent = [word]\n",
    "                # Obtenemos el tipo de entidad\n",
    "                name = tag.split('-')[1]\n",
    "                start_pos = token_index  # La posición de inicio es el token actual\n",
    "                prev_tag = tag  # Actualizamos la etiqueta del token anterior\n",
    "            elif tag.startswith('I-'):\n",
    "                # Solo agregamos la palabra actual si el token anterior tiene etiqueta I- o B-\n",
    "                if prev_tag:\n",
    "                    ent.append(word)\n",
    "                    prev_tag = tag  # Actualizamos la etiqueta del token anterior\n",
    "            elif tag == 'O' and ent:\n",
    "                # Si encontramos una etiqueta 'O' y hay una entidad en curso, la agregamos a la lista de entidades\n",
    "                end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                entitats_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Reiniciamos la lista de la entidad actual\n",
    "                ent = []\n",
    "                prev_tag = None  # Reiniciamos la etiqueta del token anterior\n",
    "\n",
    "        # Agregamos la última entidad si la hay\n",
    "        if ent:\n",
    "            end_pos = len(sentence) - 1  # La posición de fin es el último token de la oración\n",
    "            entitats_con_posiciones.add((tuple(ent), (start_pos, end_pos)))\n",
    "\n",
    "    return entitats_con_posiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_precision(entidades_referencia, entidades_extraidas):\n",
    "    # Calcular el número de entidades correctamente extraídas\n",
    "    entidades_correctas = entidades_referencia.intersection(entidades_extraidas)\n",
    "    \n",
    "    # Calcular la precisión\n",
    "    if len(entidades_extraidas) > 0:\n",
    "        precision = len(entidades_correctas) / len(entidades_extraidas)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    return precision\n",
    "\n",
    "\n",
    "def calcular_exhaustividad(entidades_referencia, entidades_extraidas):\n",
    "    # Calcular el número de entidades correctamente extraídas\n",
    "    entidades_correctas = entidades_referencia.intersection(entidades_extraidas)\n",
    "    \n",
    "    # Calcular la exhaustividad\n",
    "    if len(entidades_referencia) > 0:\n",
    "        exhaustividad = len(entidades_correctas) / len(entidades_referencia)\n",
    "    else:\n",
    "        exhaustividad = 0.0\n",
    "    \n",
    "    return exhaustividad\n",
    "\n",
    "def calcular_f1_score(precision, exhaustividad):\n",
    "    # Calcular el F1-score\n",
    "    if (precision + exhaustividad) > 0:\n",
    "        f1_score = 2 * (precision * exhaustividad) / (precision + exhaustividad)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "def resultats(predicted_BIO, testa_esp_BIO_tag):\n",
    "\n",
    "    # Obtener los conjuntos de entidades de referencia y extraídas\n",
    "    entidades_referencia = obtener_entidades_con_posiciones_prova(testa_esp_BIO_tag)  # Conjunto de entidades etiquetadas manualmente como referencia\n",
    "    entidades_extraidas =  obtener_entidades_con_posiciones_prova(predicted_BIO) # Obtener conjuntos de entidades extraídas\n",
    "\n",
    "    # Calcular la precisión\n",
    "    precision = calcular_precision(entidades_referencia, entidades_extraidas)\n",
    "\n",
    "    # Calcular la exhaustividad\n",
    "    exhaustividad = calcular_exhaustividad(entidades_referencia, entidades_extraidas)\n",
    "\n",
    "    # Calcular el F1-score\n",
    "    f1_score = calcular_f1_score(precision, exhaustividad)\n",
    "\n",
    "    print(\"Precisió:\", precision)\n",
    "    print(\"Exhaustivitat:\", exhaustividad)\n",
    "    print(\"F1-score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  1\n",
      "Precisió: 0.0\n",
      "Exhaustivitat: 0.0\n",
      "F1-score: 0.0\n",
      "--------------------------------------------------\n",
      "Model:  2\n",
      "Precisió: 0.39166666666666666\n",
      "Exhaustivitat: 0.3643410852713178\n",
      "F1-score: 0.37751004016064255\n",
      "--------------------------------------------------\n",
      "Model:  3\n",
      "Precisió: 0.6701492537313433\n",
      "Exhaustivitat: 0.6215393133997785\n",
      "F1-score: 0.6449296179258833\n",
      "--------------------------------------------------\n",
      "Model:  4\n",
      "Precisió: 0.7519742614799649\n",
      "Exhaustivitat: 0.7117940199335548\n",
      "F1-score: 0.7313326696060305\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_combinations = [\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True),\n",
    "    FeatureExtractor(True, True),\n",
    "    FeatureExtractor(True, True, True)\n",
    "]\n",
    "\n",
    "testa_esp_pre_tag = obtener_token(testa_esp)\n",
    "\n",
    "train_esp_BIO = obtener_token_BIO(train_esp)\n",
    "\n",
    "testa_esp_real = obtener_token_BIO(testa_esp)\n",
    "\n",
    "def model_entrenament(train, extractor, testa):\n",
    "    \n",
    "    model_BIO = CRFTagger(feature_func=extractor._get_features)\n",
    "    model_BIO.train(train, 'model_BIO.crf.tagger')\n",
    "    \n",
    "    predicted_BIO = model_BIO.tag_sents(testa)\n",
    "    \n",
    "    return resultats(predicted_BIO, testa_esp_real)\n",
    "i = 1\n",
    "for param in param_combinations:\n",
    "    print('Model: ', i)\n",
    "    model_entrenament(train_esp_BIO, param, testa_esp_pre_tag)\n",
    "    print('-'*50)\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True)\n",
    "\n",
    "testa_esp_real = obtener_token_BIO(testb_esp)\n",
    "testa_esp_pre_tag = obtener_token(testb_esp)\n",
    "\n",
    "model_BIO = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIO.train(train_esp_BIO, 'model_BIO.crf.tagger')\n",
    "    \n",
    "predicted_BIO = model_BIO.tag_sents(testa_esp_pre_tag)\n",
    "\n",
    "resultats(predicted_BIO, testa_esp_real)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  1\n",
      "Precisió: 0.6923076923076923\n",
      "Exhaustivitat: 0.01637852593266606\n",
      "F1-score: 0.032\n",
      "--------------------------------------------------\n",
      "Model:  2\n",
      "Precisió: 0.26930409914204007\n",
      "Exhaustivitat: 0.2570518653321201\n",
      "F1-score: 0.26303538175046554\n",
      "--------------------------------------------------\n",
      "Model:  3\n",
      "Precisió: 0.642230252968508\n",
      "Exhaustivitat: 0.5659690627843494\n",
      "F1-score: 0.6016928657799274\n",
      "--------------------------------------------------\n",
      "Model:  4\n",
      "Precisió: 0.7045231071779744\n",
      "Exhaustivitat: 0.6519563239308462\n",
      "F1-score: 0.6772211720226843\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_combinations = [\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True),\n",
    "    FeatureExtractor(True, True),\n",
    "    FeatureExtractor(True, True, True)\n",
    "]\n",
    "\n",
    "testa_ned_pre_tag = obtener_token(testa_ned)\n",
    "\n",
    "train_ned_BIO = obtener_token_BIO(train_ned)\n",
    "\n",
    "testa_ned_real = obtener_token_BIO(testa_ned)\n",
    "\n",
    "def model_entrenament(train, extractor, testa):\n",
    "    \n",
    "    model_BIO = CRFTagger(feature_func=extractor._get_features)\n",
    "    model_BIO.train(train, 'model_BIO.crf.tagger')\n",
    "    \n",
    "    predicted_BIO = model_BIO.tag_sents(testa)\n",
    "    \n",
    "    return resultats(predicted_BIO, testa_ned_real)\n",
    "i = 1\n",
    "for param in param_combinations:\n",
    "    print('Model: ', i)\n",
    "    model_entrenament(train_ned_BIO, param, testa_ned_pre_tag)\n",
    "    print('-'*50)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7329790767186981\n",
      "Exhaustivitat: 0.6759571209800919\n",
      "F1-score: 0.7033142128744423\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True)\n",
    "\n",
    "testa_ned_real = obtener_token_BIO(testb_ned)\n",
    "testa_ned_pre_tag = obtener_token(testb_ned)\n",
    "\n",
    "model_BIO = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIO.train(train_ned_BIO, 'model_BIO.crf.tagger')\n",
    "    \n",
    "predicted_BIO = model_BIO.tag_sents(testa_ned_pre_tag)\n",
    "\n",
    "resultats(predicted_BIO, testa_ned_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicció amb IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_io(train_data_bio):\n",
    "    train_data_io = []\n",
    "    for sentence in train_data_bio:\n",
    "        io_tags = []\n",
    "        for word, pos_tag, bio_tag in sentence:\n",
    "            if bio_tag == 'O':\n",
    "                io_tags.append('O')\n",
    "            elif bio_tag.startswith('B-'):\n",
    "                io_tags.append('I' + bio_tag[1:])\n",
    "            else:\n",
    "                io_tags.append(bio_tag)\n",
    "                \n",
    "        train_data_io.append(list(zip([word for word, pos_tag, bio_tag in sentence], io_tags)))\n",
    "    return train_data_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_entidades_con_posiciones_io(train_data_io):\n",
    "    entidades_con_posiciones = set()\n",
    "\n",
    "    for sentence_index, sentence in enumerate(train_data_io):\n",
    "        ent = []\n",
    "        name = None\n",
    "        start_pos = None  # Posición de inicio de la entidad actual\n",
    "        prev_tag = None  # Almacenar la etiqueta del token anterior\n",
    "\n",
    "        for token_index, (word, io_tag) in enumerate(sentence):\n",
    "            if io_tag != 'O':\n",
    "                if io_tag.startswith('I-'):\n",
    "                    ent.append(word)\n",
    "                else:\n",
    "                    # Si hay una entidad anterior, la agregamos a la lista de entidades\n",
    "                    if ent:\n",
    "                        end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                        entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                    # Creamos una nueva entidad con la palabra actual\n",
    "                    ent = [word]\n",
    "                    # Obtenemos el tipo de entidad\n",
    "                    name = io_tag.split('-')[1]\n",
    "                    start_pos = token_index  # La posición de inicio es el token actual\n",
    "                prev_tag = io_tag  # Actualizamos la etiqueta del token anterior\n",
    "            elif ent:\n",
    "                # Si encontramos una etiqueta 'O' y hay una entidad en curso, la agregamos a la lista de entidades\n",
    "                end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Reiniciamos la lista de la entidad actual\n",
    "                ent = []\n",
    "                prev_tag = None  # Reiniciamos la etiqueta del token anterior\n",
    "\n",
    "        # Agregamos la última entidad si la hay\n",
    "        if ent:\n",
    "            end_pos = len(sentence) - 1  # La posición de fin es el último token de la oración\n",
    "            entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "\n",
    "    return entidades_con_posiciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_precision(entidades_referencia, entidades_extraidas):\n",
    "    # Calcular el número de entidades correctamente extraídas\n",
    "    entidades_correctas = entidades_referencia.intersection(entidades_extraidas)\n",
    "    \n",
    "    # Calcular la precisión\n",
    "    if len(entidades_extraidas) > 0:\n",
    "        precision = len(entidades_correctas) / len(entidades_extraidas)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    return precision\n",
    "\n",
    "\n",
    "def calcular_exhaustividad(entidades_referencia, entidades_extraidas):\n",
    "    # Calcular el número de entidades correctamente extraídas\n",
    "    entidades_correctas = entidades_referencia.intersection(entidades_extraidas)\n",
    "    \n",
    "    # Calcular la exhaustividad\n",
    "    if len(entidades_referencia) > 0:\n",
    "        exhaustividad = len(entidades_correctas) / len(entidades_referencia)\n",
    "    else:\n",
    "        exhaustividad = 0.0\n",
    "    \n",
    "    return exhaustividad\n",
    "\n",
    "def calcular_f1_score(precision, exhaustividad):\n",
    "    # Calcular el F1-score\n",
    "    if (precision + exhaustividad) > 0:\n",
    "        f1_score = 2 * (precision * exhaustividad) / (precision + exhaustividad)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "def resultats(predicted_BIO, testa_esp_IO_tag):\n",
    "\n",
    "    # Obtener los conjuntos de entidades de referencia y extraídas\n",
    "    entidades_referencia = obtener_entidades_con_posiciones_io(testa_esp_IO_tag)  # Conjunto de entidades etiquetadas manualmente como referencia\n",
    "    entidades_extraidas =  obtener_entidades_con_posiciones_io(predicted_BIO) # Obtener conjuntos de entidades extraídas\n",
    "\n",
    "    # Calcular la precisión\n",
    "    precision = calcular_precision(entidades_referencia, entidades_extraidas)\n",
    "\n",
    "    # Calcular la exhaustividad\n",
    "    exhaustividad = calcular_exhaustividad(entidades_referencia, entidades_extraidas)\n",
    "\n",
    "    # Calcular el F1-score\n",
    "    f1_score = calcular_f1_score(precision, exhaustividad)\n",
    "\n",
    "    print(\"Precisió:\", precision)\n",
    "    print(\"Exhaustivitat:\", exhaustividad)\n",
    "    print(\"F1-score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  1\n",
      "Precisió: 0.0\n",
      "Exhaustivitat: 0.0\n",
      "F1-score: 0.0\n",
      "--------------------------------------------------\n",
      "Model:  2\n",
      "Precisió: 0.771102433163112\n",
      "Exhaustivitat: 0.7271954674220963\n",
      "F1-score: 0.7485056130631288\n",
      "--------------------------------------------------\n",
      "Model:  3\n",
      "Precisió: 0.8323556653288052\n",
      "Exhaustivitat: 0.7637393767705383\n",
      "F1-score: 0.7965726104299009\n",
      "--------------------------------------------------\n",
      "Model:  4\n",
      "Precisió: 0.9115566037735849\n",
      "Exhaustivitat: 0.8759206798866855\n",
      "F1-score: 0.8933834151979198\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_combinations = [\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True),\n",
    "    FeatureExtractor(True, True),\n",
    "    FeatureExtractor(True, True, True)\n",
    "]\n",
    "\n",
    "testa_esp_pre_tag = obtener_token(testa_esp)\n",
    "\n",
    "train_esp_IO = convert_to_io(train_esp)\n",
    "\n",
    "testa_esp_real = convert_to_io(testa_esp)\n",
    "\n",
    "def model_entrenament(train, extractor, testa):\n",
    "    \n",
    "    model_IO = CRFTagger(feature_func=extractor._get_features)\n",
    "    model_IO.train(train, 'model_IO.crf.tagger')\n",
    "    \n",
    "    predicted_IO = model_IO.tag_sents(testa)\n",
    "    \n",
    "    return resultats(predicted_IO, testa_esp_real)\n",
    "i = 1\n",
    "for param in param_combinations:\n",
    "    print('Model: ', i)\n",
    "    model_entrenament(train_esp_IO, param, testa_esp_pre_tag)\n",
    "    print('-'*50)\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.9248455730954015\n",
      "Exhaustivitat: 0.896242101762554\n",
      "F1-score: 0.9103192028373586\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True)\n",
    "\n",
    "testa_esp_real = convert_to_io(testb_esp)\n",
    "testa_esp_pre_tag = obtener_token(testb_esp)\n",
    "\n",
    "model_IO = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_IO.train(train_esp_IO, 'model_IO.crf.tagger')\n",
    "    \n",
    "predicted_IO = model_IO.tag_sents(testa_esp_pre_tag)\n",
    "\n",
    "resultats(predicted_IO, testa_esp_real)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  1\n",
      "Precisió: 0.0\n",
      "Exhaustivitat: 0.0\n",
      "F1-score: 0.0\n",
      "--------------------------------------------------\n",
      "Model:  2\n",
      "Precisió: 0.5677352637021716\n",
      "Exhaustivitat: 0.25884016973125884\n",
      "F1-score: 0.3555699481865285\n",
      "--------------------------------------------------\n",
      "Model:  3\n",
      "Precisió: 0.8836594394500265\n",
      "Exhaustivitat: 0.7878359264497878\n",
      "F1-score: 0.8330009970089731\n",
      "--------------------------------------------------\n",
      "Model:  4\n",
      "Precisió: 0.9229198570699336\n",
      "Exhaustivitat: 0.8524280999528524\n",
      "F1-score: 0.8862745098039216\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_combinations = [\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True),\n",
    "    FeatureExtractor(True, True),\n",
    "    FeatureExtractor(True, True, True)\n",
    "]\n",
    "\n",
    "testa_ned_pre_tag = obtener_token(testa_ned)\n",
    "\n",
    "train_ned_IO = convert_to_io(train_ned)\n",
    "\n",
    "testa_ned_real = convert_to_io(testa_ned)\n",
    "\n",
    "def model_entrenament(train, extractor, testa):\n",
    "    \n",
    "    model_IO = CRFTagger(feature_func=extractor._get_features)\n",
    "    model_IO.train(train, 'model_IO.crf.tagger')\n",
    "    \n",
    "    predicted_IO = model_IO.tag_sents(testa)\n",
    "    \n",
    "    return resultats(predicted_IO, testa_ned_real)\n",
    "i = 1\n",
    "for param in param_combinations:\n",
    "    print('Model: ', i)\n",
    "    model_entrenament(train_ned_IO, param, testa_ned_pre_tag)\n",
    "    print('-'*50)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.9179993194964273\n",
      "Exhaustivitat: 0.8575969485060394\n",
      "F1-score: 0.8867707477403451\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True)\n",
    "\n",
    "testa_ned_real = convert_to_io(testb_ned)\n",
    "testa_ned_pre_tag = obtener_token(testb_ned)\n",
    "\n",
    "model_IO = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_IO.train(train_ned_IO, 'model_IO.crf.tagger')\n",
    "    \n",
    "predicted_IO = model_IO.tag_sents(testa_ned_pre_tag)\n",
    "\n",
    "resultats(predicted_IO, testa_ned_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicció amb BIOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_biow(train_data_bio):\n",
    "    train_data_biow = []\n",
    "    for sentence in train_data_bio:\n",
    "        biow_tags = []\n",
    "        for word, pos_tag, bio_tag in sentence:\n",
    "            if bio_tag == 'O':\n",
    "                biow_tags.append('O')\n",
    "            else:\n",
    "                biow_tags.append(bio_tag + 'W')  # Añadir 'W' a todas las etiquetas\n",
    "            \n",
    "        train_data_biow.append(list(zip([word for word, pos_tag, bio_tag in sentence], biow_tags)))\n",
    "    return train_data_biow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_entidades_con_posiciones_biow(train_data_biow):\n",
    "    entidades_con_posiciones = set()\n",
    "\n",
    "    for sentence_index, sentence in enumerate(train_data_biow):\n",
    "        ent = []\n",
    "        name = None\n",
    "        start_pos = None  # Posición de inicio de la entidad actual\n",
    "        prev_tag = None  # Almacenar la etiqueta del token anterior\n",
    "\n",
    "        for token_index, (word, biow_tag) in enumerate(sentence):\n",
    "            if biow_tag != 'O':\n",
    "                if biow_tag.endswith('W'):\n",
    "                    # Eliminar 'W' del final de la etiqueta\n",
    "                    io_tag = biow_tag[:-1]\n",
    "                else:\n",
    "                    io_tag = biow_tag\n",
    "                if io_tag.startswith('I-'):\n",
    "                    ent.append(word)\n",
    "                else:\n",
    "                    # Si hay una entidad anterior, la agregamos a la lista de entidades\n",
    "                    if ent:\n",
    "                        end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                        entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                    # Creamos una nueva entidad con la palabra actual\n",
    "                    ent = [word]\n",
    "                    # Obtenemos el tipo de entidad\n",
    "                    name = io_tag.split('-')[1]\n",
    "                    start_pos = token_index  # La posición de inicio es el token actual\n",
    "                prev_tag = io_tag  # Actualizamos la etiqueta del token anterior\n",
    "            elif ent:\n",
    "                # Si encontramos una etiqueta 'O' y hay una entidad en curso, la agregamos a la lista de entidades\n",
    "                end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Reiniciamos la lista de la entidad actual\n",
    "                ent = []\n",
    "                prev_tag = None  # Reiniciamos la etiqueta del token anterior\n",
    "\n",
    "        # Agregamos la última entidad si la hay\n",
    "        if ent:\n",
    "            end_pos = len(sentence) - 1  # La posición de fin es el último token de la oración\n",
    "            entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "\n",
    "    return entidades_con_posiciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_precision(entidades_referencia, entidades_extraidas):\n",
    "    # Calcular el número de entidades correctamente extraídas\n",
    "    entidades_correctas = entidades_referencia.intersection(entidades_extraidas)\n",
    "    \n",
    "    # Calcular la precisión\n",
    "    if len(entidades_extraidas) > 0:\n",
    "        precision = len(entidades_correctas) / len(entidades_extraidas)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    return precision\n",
    "\n",
    "\n",
    "def calcular_exhaustividad(entidades_referencia, entidades_extraidas):\n",
    "    # Calcular el número de entidades correctamente extraídas\n",
    "    entidades_correctas = entidades_referencia.intersection(entidades_extraidas)\n",
    "    \n",
    "    # Calcular la exhaustividad\n",
    "    if len(entidades_referencia) > 0:\n",
    "        exhaustividad = len(entidades_correctas) / len(entidades_referencia)\n",
    "    else:\n",
    "        exhaustividad = 0.0\n",
    "    \n",
    "    return exhaustividad\n",
    "\n",
    "def calcular_f1_score(precision, exhaustividad):\n",
    "    # Calcular el F1-score\n",
    "    if (precision + exhaustividad) > 0:\n",
    "        f1_score = 2 * (precision * exhaustividad) / (precision + exhaustividad)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "def resultats(predicted_BIO, testa_esp_IO_tag):\n",
    "\n",
    "    # Obtener los conjuntos de entidades de referencia y extraídas\n",
    "    entidades_referencia = obtener_entidades_con_posiciones_biow(testa_esp_IO_tag)  # Conjunto de entidades etiquetadas manualmente como referencia\n",
    "    entidades_extraidas =  obtener_entidades_con_posiciones_biow(predicted_BIO) # Obtener conjuntos de entidades extraídas\n",
    "\n",
    "    # Calcular la precisión\n",
    "    precision = calcular_precision(entidades_referencia, entidades_extraidas)\n",
    "\n",
    "    # Calcular la exhaustividad\n",
    "    exhaustividad = calcular_exhaustividad(entidades_referencia, entidades_extraidas)\n",
    "\n",
    "    # Calcular el F1-score\n",
    "    f1_score = calcular_f1_score(precision, exhaustividad)\n",
    "\n",
    "    print(\"Precisió:\", precision)\n",
    "    print(\"Exhaustivitat:\", exhaustividad)\n",
    "    print(\"F1-score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  1\n",
      "Precisió: 0.0\n",
      "Exhaustivitat: 0.0\n",
      "F1-score: 0.0\n",
      "--------------------------------------------------\n",
      "Model:  2\n",
      "Precisió: 0.39166666666666666\n",
      "Exhaustivitat: 0.36424024356490453\n",
      "F1-score: 0.37745590133371576\n",
      "--------------------------------------------------\n",
      "Model:  3\n",
      "Precisió: 0.6693500298151461\n",
      "Exhaustivitat: 0.6213672848048712\n",
      "F1-score: 0.6444667719247883\n",
      "--------------------------------------------------\n",
      "Model:  4\n",
      "Precisió: 0.7517543859649123\n",
      "Exhaustivitat: 0.7115970107943537\n",
      "F1-score: 0.7311246978529788\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_combinations = [\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True),\n",
    "    FeatureExtractor(True, True),\n",
    "    FeatureExtractor(True, True, True)\n",
    "]\n",
    "\n",
    "testa_esp_pre_tag = obtener_token(testa_esp)\n",
    "\n",
    "train_esp_BIOW = convert_to_biow(train_esp)\n",
    "\n",
    "testa_esp_real = convert_to_biow(testa_esp)\n",
    "\n",
    "def model_entrenament(train, extractor, testa):\n",
    "    \n",
    "    model_BIOW = CRFTagger(feature_func=extractor._get_features)\n",
    "    model_BIOW.train(train, 'model_BIOW.crf.tagger')\n",
    "    \n",
    "    predicted_BIOW = model_BIOW.tag_sents(testa)\n",
    "    \n",
    "    return resultats(predicted_BIOW, testa_esp_real)\n",
    "i = 1\n",
    "for param in param_combinations:\n",
    "    print('Model: ', i)\n",
    "    model_entrenament(train_esp_BIOW, param, testa_esp_pre_tag)\n",
    "    print('-'*50)\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7885268160217244\n",
      "Exhaustivitat: 0.7656558998022412\n",
      "F1-score: 0.7769230769230768\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True)\n",
    "\n",
    "testa_esp_real = convert_to_biow(testb_esp)\n",
    "testa_esp_pre_tag = obtener_token(testb_esp)\n",
    "\n",
    "model_BIOW = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIOW.train(train_esp_BIOW, 'model_BIOW.crf.tagger')\n",
    "    \n",
    "predicted_BIOW = model_BIOW.tag_sents(testa_esp_pre_tag)\n",
    "\n",
    "resultats(predicted_BIOW, testa_esp_real)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  1\n",
      "Precisió: 0.5769230769230769\n",
      "Exhaustivitat: 0.013692377909630305\n",
      "F1-score: 0.026749888542131076\n",
      "--------------------------------------------------\n",
      "Model:  2\n",
      "Precisió: 0.2477326968973747\n",
      "Exhaustivitat: 0.2368781378366043\n",
      "F1-score: 0.24218385440970602\n",
      "--------------------------------------------------\n",
      "Model:  3\n",
      "Precisió: 0.6316606311433006\n",
      "Exhaustivitat: 0.5572797809219534\n",
      "F1-score: 0.5921435499515034\n",
      "--------------------------------------------------\n",
      "Model:  4\n",
      "Precisió: 0.6983735830458354\n",
      "Exhaustivitat: 0.6467366499315381\n",
      "F1-score: 0.6715639810426541\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_combinations = [\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True),\n",
    "    FeatureExtractor(True, True),\n",
    "    FeatureExtractor(True, True, True)\n",
    "]\n",
    "\n",
    "testa_ned_pre_tag = obtener_token(testa_ned)\n",
    "\n",
    "train_ned_BIOW = convert_to_biow(train_ned)\n",
    "\n",
    "testa_ned_real = convert_to_biow(testa_ned)\n",
    "\n",
    "def model_entrenament(train, extractor, testa):\n",
    "    \n",
    "    model_BIOW = CRFTagger(feature_func=extractor._get_features)\n",
    "    model_BIOW.train(train, 'model_BIOW.crf.tagger')\n",
    "    \n",
    "    predicted_BIOW = model_BIOW.tag_sents(testa)\n",
    "    \n",
    "    return resultats(predicted_BIOW, testa_ned_real)\n",
    "i = 1\n",
    "for param in param_combinations:\n",
    "    print('Model: ', i)\n",
    "    model_entrenament(train_ned_BIOW, param, testa_ned_pre_tag)\n",
    "    print('-'*50)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7248165443629087\n",
      "Exhaustivitat: 0.6684097200861273\n",
      "F1-score: 0.6954712754040647\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True)\n",
    "\n",
    "testa_ned_real = convert_to_biow(testb_ned)\n",
    "testa_ned_pre_tag = obtener_token(testb_ned)\n",
    "\n",
    "model_BIOW = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIOW.train(train_ned_BIOW, 'model_BIOW.crf.tagger')\n",
    "    \n",
    "predicted_BIOW = model_BIOW.tag_sents(testa_ned_pre_tag)\n",
    "\n",
    "resultats(predicted_BIOW, testa_ned_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicció amb BIOES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bioes(train_data_bio):\n",
    "    train_data_bioes = []\n",
    "    for sentence in train_data_bio:\n",
    "        bioes_tags = []\n",
    "        for i, (word, pos_tag, bio_tag) in enumerate(sentence):\n",
    "            if bio_tag == 'O':\n",
    "                bioes_tags.append('O')\n",
    "            elif bio_tag.startswith('B-'):\n",
    "                if i == len(sentence) - 1 or sentence[i + 1][2] != 'I' + bio_tag[1:]:\n",
    "                    bioes_tags.append('S' + bio_tag[1:])  # Single\n",
    "                else:\n",
    "                    bioes_tags.append('B' + bio_tag[1:])  # Begin\n",
    "            elif bio_tag.startswith('I-'):\n",
    "                if i == len(sentence) - 1 or sentence[i + 1][2] != 'I' + bio_tag[1:]:\n",
    "                    bioes_tags.append('E' + bio_tag[1:])  # End\n",
    "                else:\n",
    "                    bioes_tags.append('I' + bio_tag[1:])  # Inside\n",
    "            else:\n",
    "                raise ValueError(\"Etiqueta BIO incorrecta: {}\".format(bio_tag))\n",
    "                \n",
    "        train_data_bioes.append(list(zip([word for word, pos_tag, bio_tag in sentence], bioes_tags)))\n",
    "    return train_data_bioes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_entidades_con_posiciones_bioes(train_data_bioes):\n",
    "    entidades_con_posiciones = set()\n",
    "\n",
    "    for sentence_index, sentence in enumerate(train_data_bioes):\n",
    "        ent = []\n",
    "        name = None\n",
    "        start_pos = None  # Posición de inicio de la entidad actual\n",
    "\n",
    "        for token_index, (word, bioes_tag) in enumerate(sentence):\n",
    "            if bioes_tag != 'O':\n",
    "                if bioes_tag.startswith('B-') or bioes_tag.startswith('S-'):\n",
    "                    # Si hay una entidad anterior, la agregamos a la lista de entidades\n",
    "                    if ent:\n",
    "                        end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                        entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                    # Creamos una nueva entidad con la palabra actual\n",
    "                    ent = [word]\n",
    "                    # Obtenemos el tipo de entidad\n",
    "                    name = bioes_tag.split('-')[1]\n",
    "                    start_pos = token_index  # La posición de inicio es el token actual\n",
    "                elif bioes_tag.startswith('I-') or bioes_tag.startswith('E-'):\n",
    "                    ent.append(word)\n",
    "            elif ent:\n",
    "                # Si encontramos una etiqueta 'O' y hay una entidad en curso, la agregamos a la lista de entidades\n",
    "                end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Reiniciamos la lista de la entidad actual\n",
    "                ent = []\n",
    "\n",
    "        # Agregamos la última entidad si la hay\n",
    "        if ent:\n",
    "            end_pos = len(sentence) - 1  # La posición de fin es el último token de la oración\n",
    "            entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "\n",
    "    return entidades_con_posiciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  1\n",
      "Precisió: 0.0\n",
      "Exhaustivitat: 0.0\n",
      "F1-score: 0.0\n",
      "--------------------------------------------------\n",
      "Model:  2\n",
      "Precisió: 0.4167164179104478\n",
      "Exhaustivitat: 0.4073929961089494\n",
      "F1-score: 0.4120019675356616\n",
      "--------------------------------------------------\n",
      "Model:  3\n",
      "Precisió: 0.7058585039529698\n",
      "Exhaustivitat: 0.677431906614786\n",
      "F1-score: 0.6913531222078825\n",
      "--------------------------------------------------\n",
      "Model:  4\n",
      "Precisió: 0.7671623548584233\n",
      "Exhaustivitat: 0.7326848249027237\n",
      "F1-score: 0.7495273161508608\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_combinations = [\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True),\n",
    "    FeatureExtractor(True, True),\n",
    "    FeatureExtractor(True, True, True)\n",
    "]\n",
    "\n",
    "testa_esp_pre_tag = obtener_token(testa_esp)\n",
    "\n",
    "train_esp_BIOES = convert_to_bioes(train_esp)\n",
    "\n",
    "testa_esp_real = convert_to_bioes(testa_esp)\n",
    "\n",
    "def model_entrenament(train, extractor, testa):\n",
    "    \n",
    "    model_BIOES = CRFTagger(feature_func=extractor._get_features)\n",
    "    model_BIOES.train(train, 'model_BIOW.crf.tagger')\n",
    "    \n",
    "    predicted_BIOES = model_BIOES.tag_sents(testa)\n",
    "    \n",
    "    return resultats(predicted_BIOES, testa_esp_real)\n",
    "i = 1\n",
    "for param in param_combinations:\n",
    "    print('Model: ', i)\n",
    "    model_entrenament(train_esp_BIOES, param, testa_esp_pre_tag)\n",
    "    print('-'*50)\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7937620889748549\n",
      "Exhaustivitat: 0.7768575485092286\n",
      "F1-score: 0.7852188471657499\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True)\n",
    "\n",
    "testa_esp_real = convert_to_bioes(testb_esp)\n",
    "testa_esp_pre_tag = obtener_token(testb_esp)\n",
    "\n",
    "model_BIOES = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIOES.train(train_esp_BIOES, 'model_BIOES.crf.tagger')\n",
    "    \n",
    "predicted_BIOES = model_BIOES.tag_sents(testa_esp_pre_tag)\n",
    "\n",
    "resultats(predicted_BIOES, testa_esp_real)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  1\n",
      "Precisió: 0.6122448979591837\n",
      "Exhaustivitat: 0.0211864406779661\n",
      "F1-score: 0.040955631399317405\n",
      "--------------------------------------------------\n",
      "Model:  2\n",
      "Precisió: 0.35880022637238257\n",
      "Exhaustivitat: 0.22387005649717515\n",
      "F1-score: 0.2757121113285497\n",
      "--------------------------------------------------\n",
      "Model:  3\n",
      "Precisió: 0.6693672839506173\n",
      "Exhaustivitat: 0.6126412429378532\n",
      "F1-score: 0.6397492625368733\n",
      "--------------------------------------------------\n",
      "Model:  4\n",
      "Precisió: 0.7253925698965914\n",
      "Exhaustivitat: 0.6687853107344632\n",
      "F1-score: 0.6959397391144589\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_combinations = [\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True),\n",
    "    FeatureExtractor(True, True),\n",
    "    FeatureExtractor(True, True, True)\n",
    "]\n",
    "\n",
    "testa_ned_pre_tag = obtener_token(testa_ned)\n",
    "\n",
    "train_ned_BIOES = convert_to_bioes(train_ned)\n",
    "\n",
    "testa_ned_real = convert_to_bioes(testa_ned)\n",
    "\n",
    "def model_entrenament(train, extractor, testa):\n",
    "    \n",
    "    model_BIOES = CRFTagger(feature_func=extractor._get_features)\n",
    "    model_BIOES.train(train, 'model_BIOES.crf.tagger')\n",
    "    \n",
    "    predicted_BIOES = model_BIOES.tag_sents(testa)\n",
    "    \n",
    "    return resultats(predicted_BIOES, testa_ned_real)\n",
    "i = 1\n",
    "for param in param_combinations:\n",
    "    print('Model: ', i)\n",
    "    model_entrenament(train_ned_BIOES, param, testa_ned_pre_tag)\n",
    "    print('-'*50)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7569790628115654\n",
      "Exhaustivitat: 0.703660797034291\n",
      "F1-score: 0.7293467819404418\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True)\n",
    "\n",
    "testa_ned_real = convert_to_bioes(testb_ned)\n",
    "testa_ned_pre_tag = obtener_token(testb_ned)\n",
    "\n",
    "model_BIOES = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIOES.train(train_ned_BIOES, 'model_BIOES.crf.tagger')\n",
    "    \n",
    "predicted_BIOES = model_BIOES.tag_sents(testa_ned_pre_tag)\n",
    "\n",
    "resultats(predicted_BIOES, testa_ned_real)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

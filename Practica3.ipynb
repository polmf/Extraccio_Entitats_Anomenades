{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pycrfsuite\n",
    "from nltk.corpus import conll2002\n",
    "from nltk.tag import CRFTagger\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt') # Tokenitzador\n",
    "nltk.download('averaged_perceptron_tagger') # Etiquetador POS\n",
    "nltk.download('maxent_ne_chunker') # Etiquetador Entitats Anomenades\n",
    "nltk.download('words')\n",
    "nltk.download('treebank')\n",
    "nltk.download('conll2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9474638463198791"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = nltk.corpus.treebank.tagged_sents()[:3000]\n",
    "test = nltk.corpus.treebank.tagged_sents()[3000:]\n",
    "model.train(train, 'crfTagger.mdl')\n",
    "model.accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicció amb BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tagger = CRFTagger()\n",
    "model_BIO = CRFTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_esp = conll2002.iob_sents('esp.train') # Train, ned.train => Neerlandès\n",
    "testa_esp = conll2002.iob_sents('esp.testa') # Dev\n",
    "testb_esp = conll2002.iob_sents('esp.testb') # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9447121289420479\n"
     ]
    }
   ],
   "source": [
    "# Entrenem el model per fer els POS que corresponen a cada token\n",
    "\n",
    "train_esp_pre_tag = []\n",
    "for sentence in train_esp:\n",
    "    frases = []\n",
    "    for elem1, elem2, elem3 in sentence:\n",
    "        frases.append((elem1, elem2))\n",
    "    train_esp_pre_tag.append(frases)\n",
    "    \n",
    "model_tagger.train(train_esp_pre_tag, 'model_POS.crf.tagger')\n",
    "\n",
    "\n",
    "# Fem prediccions i mirem l'accuracy\n",
    "\n",
    "testa_esp_pre_tag = []\n",
    "for sentence in testa_esp:\n",
    "    frases = []\n",
    "    for elem1, elem2, elem3 in sentence:\n",
    "        frases.append(elem1)\n",
    "    testa_esp_pre_tag.append(frases)\n",
    "    \n",
    "predicted = model_tagger.tag_sents(testa_esp_pre_tag)\n",
    "\n",
    "predictions = [elem[1] for sentence in predicted for elem in sentence]\n",
    "real_label = [elem[1] for sentence in testa_esp for elem in sentence]\n",
    "\n",
    "print(accuracy_score(predictions, real_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, pattern):\n",
    "        self._pattern = pattern\n",
    "\n",
    "    def _get_features(self, tokens, idx):\n",
    "        \"\"\"\n",
    "        Extract basic features about this word including\n",
    "            - Current word\n",
    "            - is it capitalized?\n",
    "            - Does it have punctuation?\n",
    "            - Does it have a number?\n",
    "            - Preffixes up to length 3\n",
    "            - Suffixes up to length 3\n",
    "            - paraules prèvies i posteriors amb POS\n",
    "            - POS-tags\n",
    "            - longitud\n",
    "\n",
    "        Note that : we might include feature over previous word, next word etc.\n",
    "\n",
    "        :return: a list which contains the features\n",
    "        :rtype: list(str)\n",
    "        \"\"\"\n",
    "        token = tokens[idx]\n",
    "\n",
    "        feature_list = []\n",
    "\n",
    "        if not token:\n",
    "            return feature_list\n",
    "\n",
    "        # Capitalization\n",
    "        if token[0].isupper():\n",
    "            feature_list.append(\"CAPITALIZATION\")\n",
    "\n",
    "        # Number\n",
    "        if re.search(self._pattern, token) is not None:\n",
    "            feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "        # Punctuation\n",
    "        punc_cat = {\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"}\n",
    "        if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "            feature_list.append(\"PUNCTUATION\")\n",
    "            \n",
    "        # preffix up to length 3\n",
    "        if len(token) > 1:\n",
    "            feature_list.append(\"PRE_\" + token[:1])\n",
    "        if len(token) > 2:\n",
    "            feature_list.append(\"PRE_\" + token[:2])\n",
    "        if len(token) > 3:\n",
    "            feature_list.append(\"PRE_\" + token[:3])\n",
    "\n",
    "        # Suffix up to length 3\n",
    "        if len(token) > 1:\n",
    "            feature_list.append(\"SUF_\" + token[-1:])\n",
    "        if len(token) > 2:\n",
    "            feature_list.append(\"SUF_\" + token[-2:])\n",
    "        if len(token) > 3:\n",
    "            feature_list.append(\"SUF_\" + token[-3:])\n",
    "        \n",
    "        # POS_tags\n",
    "        POS = model_tagger.tag(tokens)\n",
    "            \n",
    "        # Paraules prèvies amb POS\n",
    "        if idx > 0:\n",
    "            feature_list.append(\"anterior1_\" + tokens[idx-1] + \"_\" + POS[idx-1][1])\n",
    "        if idx > 1:\n",
    "            feature_list.append(\"anterior2_\" + tokens[idx-2] + \"_\" + POS[idx-2][1])\n",
    "            \n",
    "        # Paraules posteriors amb POS\n",
    "        if idx < (len(tokens)-1):\n",
    "            feature_list.append(\"posterior1_\" + tokens[idx+1] + \"_\" + POS[idx+1][1])\n",
    "        if idx < (len(tokens)-2):\n",
    "            feature_list.append(\"posterior2_\" + tokens[idx+2] + \"_\" + POS[idx+2][1])\n",
    "\n",
    "        feature_list.append(\"WORD_\" + token)\n",
    "\n",
    "        return feature_list\n",
    "\n",
    "# Crear una instancia de FeatureExtractor\n",
    "pattern = r'\\d+'  # Patrón para encontrar números\n",
    "feature_extractor = FeatureExtractor(pattern)\n",
    "\n",
    "train_esp_pre_BIO = []\n",
    "for sentence in train_esp:\n",
    "    frases = []\n",
    "    for elem1, elem2, elem3 in sentence:\n",
    "        frases.append((elem1, elem3))\n",
    "    train_esp_pre_BIO.append(frases)\n",
    "    \n",
    "model_BIO = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIO.train(train_esp_pre_BIO, 'model_BIO.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9580711599871511\n"
     ]
    }
   ],
   "source": [
    "predicted_BIO = model_BIO.tag_sents(testa_esp_pre_tag)\n",
    "\n",
    "predictions_BIO = [elem[1] for sentence in predicted_BIO for elem in sentence]\n",
    "real_label_BIO = [elem[2] for sentence in testa_esp for elem in sentence]\n",
    "\n",
    "print(accuracy_score(predictions_BIO, real_label_BIO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicció amb IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_io(train_data_bio):\n",
    "    train_data_io = []\n",
    "    for sentence in train_data_bio:\n",
    "        io_tags = []\n",
    "        for word, pos_tag, bio_tag in sentence:\n",
    "            if bio_tag == 'O':\n",
    "                io_tags.append('O')\n",
    "            elif bio_tag.startswith('B-'):\n",
    "                io_tags.append('I' + bio_tag[1:])\n",
    "            else:\n",
    "                io_tags.append(bio_tag)\n",
    "                \n",
    "        train_data_io.append(list(zip([word for word, pos_tag, bio_tag in sentence], io_tags)))\n",
    "    return train_data_io\n",
    "\n",
    "train_esp_pre_IO = convert_to_io(train_esp)\n",
    "\n",
    "# Entrenar el modelo CRFTagger con el esquema IO\n",
    "model_IO = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_IO.train(train_esp_pre_IO, 'model_io.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9559170870887894\n"
     ]
    }
   ],
   "source": [
    "testa_pre_IO = convert_to_io(testa_esp)\n",
    "\n",
    "predicted_IO = model_IO.tag_sents(testa_esp_pre_tag)\n",
    "\n",
    "predictions_IO = [elem[1] for sentence in predicted_IO for elem in sentence]\n",
    "real_label_IO = [elem[1] for sentence in testa_pre_IO for elem in sentence]\n",
    "\n",
    "print(accuracy_score(predictions_IO, real_label_IO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mark', 'B-PER'),\n",
       " ('Pedersen', 'I-PER'),\n",
       " ('treballa', 'O'),\n",
       " ('a', 'O'),\n",
       " ('Google', 'B-LOC'),\n",
       " ('des', 'O'),\n",
       " ('del', 'O'),\n",
       " ('1994', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_BIO.tag(nltk.word_tokenize(\"Mark Pedersen treballa a Google des del 1994.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mark', 'I-PER'),\n",
       " ('Pedersen', 'I-PER'),\n",
       " ('treballa', 'O'),\n",
       " ('a', 'O'),\n",
       " ('Google', 'I-PER'),\n",
       " ('des', 'O'),\n",
       " ('del', 'O'),\n",
       " ('1994', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_IO.tag(nltk.word_tokenize(\"Mark Pedersen treballa a Google des del 1994.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaluació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9459214330253387"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avaluació mal feta, contant només quants tokens són correctes, i no les entitats correctes.\n",
    "model.accuracy(testa_esp_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaluació ben feta:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hem d'avaluar quantes entitats estan reconegudes correctament, no quants tokens son correctes.\n",
    "Descodificar la sequencia i obtenir les entitats, i doncs avaluar les entitats.\n",
    "Per exemple, 'Mark Pedersen Romero' --> 'M P R' (una entitat) per BIO; 'M' i 'P R' (dos entitats) per IO; en aquest exemple IO ho fa malament.\n",
    "\n",
    "A nivell d'entitats: Recall i f-score\n",
    "\n",
    "Per avaluar el model avaluem en base a recall i precisio parcial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple d'ús CRFTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: El, Features: ['CAPITALIZATION', 'PRE_E', 'SUF_l', 'posterior1_men_NC', 'posterior2_atendió_VMI', 'WORD_El']\n",
      "Token: men, Features: ['PRE_m', 'PRE_me', 'SUF_n', 'SUF_en', 'anterior1_El_DA', 'posterior1_atendió_VMI', 'posterior2_a_SP', 'WORD_men']\n",
      "Token: atendió, Features: ['PRE_a', 'PRE_at', 'PRE_ate', 'SUF_ó', 'SUF_ió', 'SUF_dió', 'anterior1_men_NC', 'anterior2_El_DA', 'posterior1_a_SP', 'posterior2_la_DA', 'WORD_atendió']\n",
      "Token: a, Features: ['anterior1_atendió_VMI', 'anterior2_men_NC', 'posterior1_la_DA', 'posterior2_reunión_NC', 'WORD_a']\n",
      "Token: la, Features: ['PRE_l', 'SUF_a', 'anterior1_a_SP', 'anterior2_atendió_VMI', 'posterior1_reunión_NC', 'WORD_la']\n",
      "Token: reunión, Features: ['PRE_r', 'PRE_re', 'PRE_reu', 'SUF_n', 'SUF_ón', 'SUF_ión', 'anterior1_la_DA', 'anterior2_a_SP', 'WORD_reunión']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, pattern):\n",
    "        self._pattern = pattern\n",
    "\n",
    "    def _get_features(self, tokens, idx):\n",
    "        \"\"\"\n",
    "        Extract basic features about this word including\n",
    "            - Current word\n",
    "            - is it capitalized?\n",
    "            - Does it have punctuation?\n",
    "            - Does it have a number?\n",
    "            - Preffixes up to length 3\n",
    "            - Suffixes up to length 3\n",
    "            - paraules prèvies i posteriors amb POS\n",
    "            - POS-tags\n",
    "            - longitud\n",
    "\n",
    "        Note that : we might include feature over previous word, next word etc.\n",
    "\n",
    "        :return: a list which contains the features\n",
    "        :rtype: list(str)\n",
    "        \"\"\"\n",
    "        token = tokens[idx]\n",
    "\n",
    "        feature_list = []\n",
    "\n",
    "        if not token:\n",
    "            return feature_list\n",
    "\n",
    "        # Capitalization\n",
    "        if token[0].isupper():\n",
    "            feature_list.append(\"CAPITALIZATION\")\n",
    "\n",
    "        # Number\n",
    "        if re.search(self._pattern, token) is not None:\n",
    "            feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "        # Punctuation\n",
    "        punc_cat = {\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"}\n",
    "        if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "            feature_list.append(\"PUNCTUATION\")\n",
    "            \n",
    "        # preffix up to length 3\n",
    "        if len(token) > 1:\n",
    "            feature_list.append(\"PRE_\" + token[:1])\n",
    "        if len(token) > 2:\n",
    "            feature_list.append(\"PRE_\" + token[:2])\n",
    "        if len(token) > 3:\n",
    "            feature_list.append(\"PRE_\" + token[:3])\n",
    "\n",
    "        # Suffix up to length 3\n",
    "        if len(token) > 1:\n",
    "            feature_list.append(\"SUF_\" + token[-1:])\n",
    "        if len(token) > 2:\n",
    "            feature_list.append(\"SUF_\" + token[-2:])\n",
    "        if len(token) > 3:\n",
    "            feature_list.append(\"SUF_\" + token[-3:])\n",
    "        \n",
    "        # POS_tags\n",
    "        POS = model_tagger.tag(tokens)\n",
    "            \n",
    "        # Paraules prèvies amb POS\n",
    "        if idx > 0:\n",
    "            feature_list.append(\"anterior1_\" + tokens[idx-1] + \"_\" + POS[idx-1][1])\n",
    "        if idx > 1:\n",
    "            feature_list.append(\"anterior2_\" + tokens[idx-2] + \"_\" + POS[idx-2][1])\n",
    "            \n",
    "        # Paraules posteriors amb POS\n",
    "        if idx < (len(tokens)-1):\n",
    "            feature_list.append(\"posterior1_\" + tokens[idx+1] + \"_\" + POS[idx+1][1])\n",
    "        if idx < (len(tokens)-2):\n",
    "            feature_list.append(\"posterior2_\" + tokens[idx+2] + \"_\" + POS[idx+2][1])\n",
    "\n",
    "        feature_list.append(\"WORD_\" + token)\n",
    "\n",
    "        return feature_list\n",
    "\n",
    "# Ejemplo de uso:\n",
    "pattern = r'\\d+'  # Patrón para encontrar números\n",
    "feature_extractor = FeatureExtractor(pattern)\n",
    "\n",
    "tokens = ['El', 'men', 'atendió', 'a', 'la', 'reunión']\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    features = feature_extractor._get_features(tokens, i)\n",
    "    print(f\"Token: {token}, Features: {features}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

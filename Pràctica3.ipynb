{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:lightblue; font-weight:bold;\">EXTRACCIÓ D'ENTITATS ANOMENADES</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ADD8E6; font-weight:bold;\">Índex:</span>\n",
    "### <span style=\"color:#FFFFFF;\">1. [Predicció amb BIO](#bio)</span>\n",
    "### <span style=\"color:#FFFFFF;\">2. [Predicció amb IO](#io)</span>\n",
    "### <span style=\"color:#ADD8E6;\">3. [Predicció amb BIOES](#bioes)</span>\n",
    "### <span style=\"color:#ADD8E6;\">4. [Conclusions](#conclusions)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue;\"> Imports i Descarregues </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pycrfsuite\n",
    "from nltk.corpus import conll2002\n",
    "from nltk.tag import CRFTagger\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt') # Tokenitzador\n",
    "nltk.download('averaged_perceptron_tagger') # Etiquetador POS\n",
    "nltk.download('maxent_ne_chunker') # Etiquetador Entitats Anomenades\n",
    "nltk.download('words')\n",
    "nltk.download('treebank')\n",
    "nltk.download('conll2002')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue;\"> Inicialització </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquesta secció obtenim els conjunts que utilitzarem més endavant per entrenar i avaluar els nostres models de reconeixement d'entitats anomenades. \n",
    "A més, definim les funcions que utilitzarem a posteriori per automatitzar procesos repetitius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_esp = conll2002.iob_sents('esp.train') # Train\n",
    "testa_esp = conll2002.iob_sents('esp.testa') # Dev\n",
    "testb_esp = conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "testa_ned = conll2002.iob_sents('ned.testa') # Dev\n",
    "testb_ned = conll2002.iob_sents('ned.testb') # Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim les funcions per a obtenir diferents configuracions de representació dels nostres conjunts a entrenar i avaluar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenir_token_POS(fitxer):\n",
    "    \"\"\"\n",
    "    Funció per convertir un text amb el token, POS i entitat per \n",
    "    cada element en cada frase en un text amb el token i el seu POS\n",
    "    per cada element.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for sentence in fitxer:\n",
    "        frases = []\n",
    "        for elem1, elem2, elem3 in sentence:\n",
    "            frases.append((elem1, elem2))\n",
    "        res.append(frases)\n",
    "    return res\n",
    "\n",
    "# Fem prediccions i mirem l'accuracy\n",
    "def obtenir_token(fitxer):\n",
    "    \"\"\"\n",
    "    Funció per convertir un text amb el token, POS i entitat \n",
    "    per cada element en cada frase en un text amb només el token\n",
    "    per cada element.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for sentence in fitxer:\n",
    "        frases = []\n",
    "        for elem1, elem2, elem3 in sentence:\n",
    "            frases.append(elem1)\n",
    "        res.append(frases)\n",
    "    return res\n",
    "\n",
    "\n",
    "def obtenir_token_entity(fitxer):\n",
    "    \"\"\"\n",
    "    Funció per convertir un text amb el token, POS i entitat per cada \n",
    "    element en cada frase en un text amb el token i la seva entitat per \n",
    "    cada element.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for sentence in fitxer:\n",
    "        frases = []\n",
    "        for elem1, elem2, elem3 in sentence:\n",
    "            frases.append((elem1, elem3))\n",
    "        res.append(frases)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightblue;\"> Classe FeatureExtractor personalitzada </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span> Features que es tenen en compte:\n",
    "<ul>\n",
    "    <li>Paraula actual</li>\n",
    "    <li>Si comença en majúscula</li>\n",
    "    <li>Si té signe de puntuació</li>\n",
    "    <li>Si té números</li>\n",
    "    <li>Prefixos fins a longitud 3</li>\n",
    "    <li>Sufixos fins a longitud 3</li>\n",
    "    <li>Paraules prèvies i posteriors amb POS</li>\n",
    "    <li>POS-tags</li>\n",
    "    <li>Longitud de la paraula</li>\n",
    "</ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \n",
    "    \"\"\"\n",
    "    Aquesta classe conté el mètode per calcular les features\n",
    "    que s'utilitzaran per entrenar el model més endavant.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_basic_features=False, use_prefix_suffix_features=False, use_context_features=False, pattern = r'\\d+', model_POS = None):\n",
    "        self.use_basic_features = use_basic_features\n",
    "        self.use_prefix_suffix_features = use_prefix_suffix_features\n",
    "        self.use_context_features = use_context_features\n",
    "        self._pattern = pattern\n",
    "        self.model_POS = model_POS\n",
    "        \n",
    "    def __str__(self):\n",
    "        features = []\n",
    "        if self.use_basic_features:\n",
    "            features.append(\"use_basic_features=True\")\n",
    "        if self.use_prefix_suffix_features:\n",
    "            features.append(\"use_prefix_suffix_features=True\")\n",
    "        if self.use_context_features:\n",
    "            features.append(\"use_context_features=True\")\n",
    "        return f\"FeatureExtractor({', '.join(features)})\"\n",
    "        \n",
    "    def _get_features(self, tokens, idx):\n",
    "        \"\"\"\n",
    "        Extract basic features about this word including\n",
    "            - Current word\n",
    "            - is it capitalized?\n",
    "            - Does it have punctuation?\n",
    "            - Does it have a number?\n",
    "            - Preffixes up to length 3\n",
    "            - Suffixes up to length 3\n",
    "            - paraules prèvies i posteriors amb POS\n",
    "            - POS-tags\n",
    "            - longitud\n",
    "\n",
    "        Note that : we might include feature over previous word, next word etc.\n",
    "\n",
    "        :return: a list which contains the features\n",
    "        :rtype: list(str)\n",
    "        \"\"\"\n",
    "            \n",
    "        token = tokens[idx]\n",
    "        \n",
    "        feature_list = []\n",
    "        \n",
    "        if self.use_basic_features:\n",
    "            # Capitalization\n",
    "            if token[0].isupper():\n",
    "                feature_list.append(\"CAPITALIZATION\")\n",
    "\n",
    "            # Number\n",
    "            if re.search(self._pattern, token) is not None:\n",
    "                feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "            # Punctuation\n",
    "            punc_cat = {\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"}\n",
    "            if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "                feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "                    \n",
    "        if self.use_prefix_suffix_features:\n",
    "            # preffix up to length 3\n",
    "            if len(token) > 1:\n",
    "                feature_list.append(\"PRE_\" + token[:1])\n",
    "            if len(token) > 2:\n",
    "                feature_list.append(\"PRE_\" + token[:2])\n",
    "            if len(token) > 3:\n",
    "                feature_list.append(\"PRE_\" + token[:3])\n",
    "\n",
    "            # Suffix up to length 3\n",
    "            if len(token) > 1:\n",
    "                feature_list.append(\"SUF_\" + token[-1:])\n",
    "            if len(token) > 2:\n",
    "                feature_list.append(\"SUF_\" + token[-2:])\n",
    "            if len(token) > 3:\n",
    "                feature_list.append(\"SUF_\" + token[-3:])\n",
    "    \n",
    "        \n",
    "        if self.use_context_features:\n",
    "            # POS_tags\n",
    "            POS = self.model_POS.tag(tokens)\n",
    "                \n",
    "            # Paraules prèvies amb POS\n",
    "            if idx > 0:\n",
    "                feature_list.append(\"anterior1_\" + tokens[idx-1] + \"_\" + POS[idx-1][1])\n",
    "            if idx > 1:\n",
    "                feature_list.append(\"anterior2_\" + tokens[idx-2] + \"_\" + POS[idx-2][1])\n",
    "                \n",
    "            # Paraules posteriors amb POS\n",
    "            if idx < (len(tokens)-1):\n",
    "                feature_list.append(\"posterior1_\" + tokens[idx+1] + \"_\" + POS[idx+1][1])\n",
    "            if idx < (len(tokens)-2):\n",
    "                feature_list.append(\"posterior2_\" + tokens[idx+2] + \"_\" + POS[idx+2][1])\n",
    "\n",
    "            feature_list.append(\"WORD_\" + token + \"_\" + POS[idx][1])\n",
    "            \n",
    "        \n",
    "        if not self.use_context_features:\n",
    "            feature_list.append(\"WORD_\" + token)\n",
    "            \n",
    "        \n",
    "        return feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightblue;\"> Mètriques d'avaluació </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_precisio(entitats_referencia, entitats_predites):\n",
    "    # Calcular el número de entidades correctamente extraídas\n",
    "    entitats_correctes = entitats_referencia.intersection(entitats_predites)\n",
    "    \n",
    "    # Calcular la precisión\n",
    "    if len(entitats_predites) > 0:\n",
    "        precisio = len(entitats_correctes) / len(entitats_predites)\n",
    "    else:\n",
    "        precisio = 0.0\n",
    "    \n",
    "    return precisio\n",
    "\n",
    "\n",
    "def calcular_recall(entitats_referencia, entitats_predites):\n",
    "    # Calcular el número de entidades correctamente extraídas\n",
    "    entitats_correctes = entitats_referencia.intersection(entitats_predites)\n",
    "    \n",
    "    # Calcular recall\n",
    "    if len(entitats_referencia) > 0:\n",
    "        recall = len(entitats_correctes) / len(entitats_referencia)\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def calcular_f1_score(precisio, recall):\n",
    "    # Calcular el F1-score\n",
    "    if (precisio + recall) > 0:\n",
    "        f1_score = 2 * (precisio * recall) / (precisio + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "def resultats(predicted_BIO, testa_esp_BIO_tag, obtain_entity):\n",
    "\n",
    "    # Obtener los conjuntos de entidades de referencia y extraídas\n",
    "    entitats_referencia = obtain_entity(testa_esp_BIO_tag)  # Conjunto de entidades etiquetadas manualmente como referencia\n",
    "    entitats_predites =  obtain_entity(predicted_BIO) # Obtener conjuntos de entidades predichas\n",
    "\n",
    "    # Calcular la precisión\n",
    "    precisio = calcular_precisio(entitats_referencia, entitats_predites)\n",
    "\n",
    "    # Calcular la exhaustividad\n",
    "    recall = calcular_recall(entitats_referencia, entitats_predites)\n",
    "\n",
    "    # Calcular el F1-score\n",
    "    f1_score = calcular_f1_score(precisio, recall)\n",
    "\n",
    "    print(\"Precisió:\", precisio)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue; font-weight:bold;\">Predicció amb BIO</span> <a id=\"bio\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenir_entitats_amb_posicions_BIO(fitxer_BIO_tag):\n",
    "    \"\"\"\n",
    "    Funció per agrupar en sets les entitats anomenades, l'índex on comença i l'índex on acaba,\n",
    "    i la classe de la entitat.\n",
    "    \n",
    "    Argument: un text amb una tupla (amb el token i la seva entitat) per cada element en cada frase.\n",
    "    \"\"\"\n",
    "    \n",
    "    entitats_amb_posicions = set()\n",
    "\n",
    "    for sentence_index, sentence in enumerate(fitxer_BIO_tag):\n",
    "        ent = []\n",
    "        name = None\n",
    "        start_pos = None  # Posición de inicio de la entidad actual\n",
    "        prev_tag = None  # Almacenar la etiqueta del token anterior\n",
    "\n",
    "        for token_index, token in enumerate(sentence):\n",
    "            word, tag = token\n",
    "            #word = word[0]\n",
    "\n",
    "            if tag.startswith('B-'):\n",
    "                # Si hay una entidad anterior, la agregamos a la lista de entidades\n",
    "                if ent:\n",
    "                    end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                    entitats_amb_posicions.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Creamos una nueva entidad con la palabra actual\n",
    "                ent = [word]\n",
    "                # Obtenemos el tipo de entidad\n",
    "                name = tag.split('-')[1]\n",
    "                start_pos = token_index  # La posición de inicio es el token actual\n",
    "                prev_tag = tag  # Actualizamos la etiqueta del token anterior\n",
    "            elif tag.startswith('I-'):\n",
    "                # Solo agregamos la palabra actual si el token anterior tiene etiqueta I- o B-\n",
    "                if prev_tag:\n",
    "                    ent.append(word)\n",
    "                    prev_tag = tag  # Actualizamos la etiqueta del token anterior\n",
    "            elif tag == 'O' and ent:\n",
    "                # Si encontramos una etiqueta 'O' y hay una entidad en curso, la agregamos a la lista de entidades\n",
    "                end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                entitats_amb_posicions.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Reiniciamos la lista de la entidad actual\n",
    "                ent = []\n",
    "                prev_tag = None  # Reiniciamos la etiqueta del token anterior\n",
    "\n",
    "        # Agregamos la última entidad si la hay\n",
    "        if ent:\n",
    "            end_pos = len(sentence) - 1  # La posición de fin es el último token de la oración\n",
    "            entitats_amb_posicions.add((tuple(ent), (start_pos, end_pos)))\n",
    "\n",
    "    return entitats_amb_posicions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencem entrenant un model CRFTagger per a la predicció de les etiquetes POS, ja que serà utilitzat posteriorment per a calcular els features necessaris en el procés d'extracció d'entitats anomenades. Inicialment, vam considerar utilitzar les etiquetes POS existents en els textos com a dades d'entrada, però vam enfrontar-nos a un error durant el procés d'entrenament del model CRFTagger. Aquest error indicava que el model esperava rebre només un element amb la seva etiqueta, en comptes de dos elements amb una etiqueta, ja que estàvem passant una tupla que contenia tant el token com el seu POS, juntament amb l'etiqueta de l'entitat. Per tant, per evitar aquest problema, vam optar per entrenar un model separat per a la predicció de les etiquetes POS, que posteriorment seran utilitzades com a característiques addicionals en el procés d'entrenament per a l'extracció d'entitats anomenades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veiem que l'accuracy del model és prou elevada per tenir-lo en compte i utilitzar-lo més endavant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9447121289420479\n"
     ]
    }
   ],
   "source": [
    "model_tagger_POS_esp = CRFTagger()\n",
    "\n",
    "# Entrenem el model per predir els POS que corresponen a cada token\n",
    "\n",
    "train_esp_pos_tag = obtenir_token_POS(train_esp)\n",
    "    \n",
    "model_tagger_POS_esp.train(train_esp_pos_tag, 'model_POS_esp.crf.tagger')    \n",
    "\n",
    "\n",
    "testa_esp_pre_tag = obtenir_token(testa_esp)\n",
    "    \n",
    "predicted = model_tagger_POS_esp.tag_sents(testa_esp_pre_tag)\n",
    "\n",
    "predictions = [elem[1] for sentence in predicted for elem in sentence]\n",
    "real_label = [elem[1] for sentence in testa_esp for elem in sentence]\n",
    "\n",
    "print(accuracy_score(predictions, real_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuació, realitzem experiments amb diverses features per determinar quines proporcionen un rendiment òptim pel model amb etiquetatge BIO, i en textos escrits en espanyol. A més, aquests experiments els executem en el conjunt de proves \"testa\", que considerem com a conjunt de validació per ajustar i identificar les millors característiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: None\n",
      "Precisió: 0.6927890345649583\n",
      "Recall: 0.643687707641196\n",
      "F1-score: 0.6673363949483353\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor()\n",
      "Precisió: 0.718299164768413\n",
      "Recall: 0.2619047619047619\n",
      "F1-score: 0.3838506796510448\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor(use_basic_features=True)\n",
      "Precisió: 0.6447638603696099\n",
      "Recall: 0.6085271317829457\n",
      "F1-score: 0.626121635094716\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True)\n",
      "Precisió: 0.7015675835551612\n",
      "Recall: 0.6566998892580288\n",
      "F1-score: 0.6783926783926784\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True, use_context_features=True)\n",
      "Precisió: 0.7475160724722385\n",
      "Recall: 0.7081949058693244\n",
      "F1-score: 0.7273244242251918\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "param_combinations_esp = [\n",
    "    None,\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True, model_POS=model_tagger_POS_esp),\n",
    "    FeatureExtractor(True, True, model_POS=model_tagger_POS_esp),\n",
    "    FeatureExtractor(True, True, True, model_POS=model_tagger_POS_esp)\n",
    "]\n",
    "\n",
    "train_esp_BIO = obtenir_token_entity(train_esp)\n",
    "\n",
    "testa_esp_real = obtenir_token_entity(testa_esp)\n",
    "\n",
    "def model_entrenament(train_esp_BIO_tag, extractor, testa_esp_pre_tag):\n",
    "    if extractor == None:\n",
    "        model_BIO = CRFTagger()\n",
    "        model_BIO.train(train_esp_BIO_tag, 'model_BIO.crf.tagger')\n",
    "        \n",
    "        predicted_BIO = model_BIO.tag_sents(testa_esp_pre_tag)\n",
    "    \n",
    "    else:\n",
    "        model_BIO = CRFTagger(feature_func=extractor._get_features)\n",
    "        model_BIO.train(train_esp_BIO_tag, 'model_BIO.crf.tagger')\n",
    "        \n",
    "        predicted_BIO = model_BIO.tag_sents(testa_esp_pre_tag)\n",
    "    \n",
    "    return resultats(predicted_BIO, testa_esp_real, obtenir_entitats_amb_posicions_BIO)\n",
    "\n",
    "for param in param_combinations_esp:\n",
    "    print(f'Model: {str(param)}') if param != None else print(f'Model: Features per defecte de CRFTagger')\n",
    "    model_entrenament(train_esp_BIO, param, testa_esp_pre_tag)\n",
    "    print('-'*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observem que els millors resultats es donen amb el model que usa els 3 'features' diferents i, per tant, usarem aquest model per a poder identificar les entitats correctament en el conjunt de test que no hem utilitzat fins ara, el 'testb'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7869019341703427\n",
      "Recall: 0.7645895153313551\n",
      "F1-score: 0.7755852842809364\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True, model_POS=model_tagger_POS_esp)\n",
    "\n",
    "testb_esp_real = obtenir_token_entity(testb_esp)\n",
    "testb_esp_pre_tag = obtenir_token(testb_esp)\n",
    "\n",
    "model_BIO_esp = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIO_esp.train(train_esp_BIO, 'model_BIO.crf.tagger')\n",
    "    \n",
    "predicted_BIO = model_BIO_esp.tag_sents(testb_esp_pre_tag)\n",
    "\n",
    "resultats(predicted_BIO, testb_esp_real, obtenir_entitats_amb_posicions_BIO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model de CRF per predir els POS tags però ara pel nederlandés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.940191577997718\n"
     ]
    }
   ],
   "source": [
    "model_tagger_POS_ned = CRFTagger()\n",
    "\n",
    "# Entrenem el model per predir els POS que corresponen a cada token\n",
    "train_ned_pos_tag = obtenir_token_POS(train_ned)\n",
    "    \n",
    "model_tagger_POS_ned.train(train_ned_pos_tag, 'model_POS_ned.crf.tagger')\n",
    "\n",
    "\n",
    "# Fem prediccions i mirem l'accuracy\n",
    "testa_ned_pre_tag = obtenir_token(testa_ned)\n",
    "    \n",
    "predicted = model_tagger_POS_ned.tag_sents(testa_ned_pre_tag)\n",
    "\n",
    "predictions = [elem[1] for sentence in predicted for elem in sentence]\n",
    "real_label = [elem[1] for sentence in testa_ned for elem in sentence]\n",
    "\n",
    "print(accuracy_score(predictions, real_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentació amb diferents 'features' per a veure quins són els més adequats pel model amb 'BIO' i la llengua neerlandesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: None\n",
      "Precisió: 0.6441908713692946\n",
      "Recall: 0.565059144676979\n",
      "F1-score: 0.602035870092099\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor()\n",
      "Precisió: 0.762114537444934\n",
      "Recall: 0.15741583257506825\n",
      "F1-score: 0.2609351432880845\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor(use_basic_features=True)\n",
      "Precisió: 0.655096011816839\n",
      "Recall: 0.4035486806187443\n",
      "F1-score: 0.49943693693693697\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True)\n",
      "Precisió: 0.6853182751540041\n",
      "Recall: 0.6073703366696998\n",
      "F1-score: 0.6439942112879885\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True, use_context_features=True)\n",
      "Precisió: 0.7082306554953179\n",
      "Recall: 0.6537761601455869\n",
      "F1-score: 0.6799148332150462\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "param_combinations_ned = [\n",
    "    None,\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True, model_POS=model_tagger_POS_ned),\n",
    "    FeatureExtractor(True, True, model_POS=model_tagger_POS_ned),\n",
    "    FeatureExtractor(True, True, True, model_POS=model_tagger_POS_ned)\n",
    "]\n",
    "\n",
    "train_ned_BIO = obtenir_token_entity(train_ned)\n",
    "\n",
    "testa_ned_real = obtenir_token_entity(testa_ned)\n",
    "\n",
    "def model_entrenament(train_BIO_tag, extractor, testa_pre_tag):\n",
    "    if extractor == None:\n",
    "        model_BIO = CRFTagger()\n",
    "        model_BIO.train(train_BIO_tag, 'model_BIO_ned.crf.tagger')\n",
    "        \n",
    "        predicted_BIO = model_BIO.tag_sents(testa_pre_tag)\n",
    "    \n",
    "    else:    \n",
    "        model_BIO = CRFTagger(feature_func=extractor._get_features)\n",
    "        model_BIO.train(train_BIO_tag, 'model_BIO_ned.crf.tagger')\n",
    "        \n",
    "        predicted_BIO = model_BIO.tag_sents(testa_pre_tag)\n",
    "    \n",
    "    return resultats(predicted_BIO, testa_ned_real, obtenir_entitats_amb_posicions_BIO)\n",
    "\n",
    "for param in param_combinations_ned:\n",
    "    print(f'Model: {str(param)}') if param != None else print(f'Model: Features per defecte de CRFTagger')\n",
    "    model_entrenament(train_ned_BIO, param, testa_ned_pre_tag)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observem que els millors resultats es donen amb el model que usa els 3 'features' diferents i, per tant, usarem aquest model per a poder identificar les entitats correctament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7402511566424322\n",
      "Recall: 0.6860643185298622\n",
      "F1-score: 0.7121284374503258\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True, model_POS=model_tagger_POS_ned)\n",
    "\n",
    "testb_ned_real = obtenir_token_entity(testb_ned)\n",
    "testb_ned_pre_tag = obtenir_token(testb_ned)\n",
    "\n",
    "model_BIO_ned = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIO_ned.train(train_ned_BIO, 'model_BIO_ned.crf.tagger')\n",
    "    \n",
    "predicted_BIO = model_BIO_ned.tag_sents(testb_ned_pre_tag)\n",
    "\n",
    "resultats(predicted_BIO, testb_ned_real, obtenir_entitats_amb_posicions_BIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue; font-weight:bold;\">Predicció amb IO</span> <a id=\"io\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim una funció per passar de l'etiquetatge 'BIO' a l'etiquetatge 'IO'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_io(train_data_bio):\n",
    "    \"\"\"\n",
    "    Función para convertir datos de formato BIO a formato IO.\n",
    "    Argumento:\n",
    "    train_data_bio: una lista de frases, donde cada frase es una lista de tuplas (palabra, etiqueta POS, etiqueta BIO).\n",
    "    Retorna:\n",
    "    Una lista de frases en formato IO, donde cada frase es una lista de tuplas (palabra, etiqueta POS, etiqueta IO).\n",
    "    \"\"\"\n",
    "    train_data_io = []\n",
    "    for sentence in train_data_bio:\n",
    "        io_tags = []\n",
    "        for word, pos_tag, bio_tag in sentence:\n",
    "            if bio_tag == 'O':\n",
    "                io_tags.append('O')\n",
    "            elif bio_tag.startswith('B-'):\n",
    "                io_tags.append('I' + bio_tag[1:])\n",
    "            else:\n",
    "                io_tags.append(bio_tag)\n",
    "        \n",
    "        train_data_io.append([(word, pos_tag, io_tag) for (word, pos_tag, bio_tag), io_tag in zip(sentence, io_tags)])\n",
    "    return train_data_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquesta funció agrupa les entitats anomenades amb les seves posicions d'inici i fi, així com la seva classe, en un conjunt de tuples. Cada tuple conté tres elements: la llista de tokens que formen l'entitat, una tupla amb les posicions d'inici i fi (els índexs) de l'entitat dins del text, i la classe de l'entitat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenir_entitats_amb_posicions_IO(fitxer_IO_tag):\n",
    "    \"\"\"\n",
    "    Función para agrupar en sets las entidades nombradas, el índice donde empieza y el índice donde acaba,\n",
    "    y la clase de la entidad.\n",
    "    \n",
    "    Argumento: un texto con una tupla (con el token y su entidad) para cada elemento en cada frase.\n",
    "    \"\"\"\n",
    "    \n",
    "    entitats_amb_posicions = set()\n",
    "\n",
    "    for sentence_index, sentence in enumerate(fitxer_IO_tag):\n",
    "        ent = []\n",
    "        name = None\n",
    "        start_pos = None  # Posición de inicio de la entidad actual\n",
    "        prev_tag = None  # Almacenar la etiqueta del token anterior\n",
    "\n",
    "        for token_index, token in enumerate(sentence):\n",
    "            word, tag = token\n",
    "\n",
    "            if tag.startswith('I-'):\n",
    "                # Si la etiqueta es 'I-' y el token anterior es 'O', comenzamos una nueva entidad\n",
    "                if not prev_tag:\n",
    "                    # Si hay una entidad anterior, la agregamos a la lista de entidades\n",
    "                    if ent:\n",
    "                        end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                        entitats_amb_posicions.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                    # Creamos una nueva entidad con la palabra actual\n",
    "                    ent = [word]\n",
    "                    # Obtenemos el tipo de entidad\n",
    "                    name = tag.split('-')[1]\n",
    "                    start_pos = token_index  # La posición de inicio es el token actual\n",
    "                else:\n",
    "                    # Si el token anterior es 'I-', agregamos la palabra actual a la entidad en curso\n",
    "                    ent.append(word)\n",
    "                prev_tag = tag  # Actualizamos la etiqueta del token anterior\n",
    "            \n",
    "            elif tag == 'O' and ent:\n",
    "                # Si encontramos una etiqueta 'O' y hay una entidad en curso, la agregamos a la lista de entidades\n",
    "                end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                entitats_amb_posicions.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Reiniciamos la lista de la entidad actual\n",
    "                ent = []\n",
    "                prev_tag = None  # Reiniciamos la etiqueta del token anterior\n",
    "\n",
    "        # Agregamos la última entidad si la hay\n",
    "        if ent:\n",
    "            end_pos = len(sentence) - 1  # La posición de fin es el último token de la oración\n",
    "            entitats_amb_posicions.add((tuple(ent), (start_pos, end_pos), name))\n",
    "\n",
    "    return entitats_amb_posicions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentació amb diferents 'features' per a veure quins són els més adequats pel model amb 'IO' i la llengua espanyola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: None\n",
      "Precisió: 0.6720413751140858\n",
      "Recall: 0.6208544125913434\n",
      "F1-score: 0.6454346238130021\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor()\n",
      "Precisió: 0.6519083969465649\n",
      "Recall: 0.24002248454187747\n",
      "F1-score: 0.35086277732128185\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True)\n",
      "Precisió: 0.6396209653538644\n",
      "Recall: 0.6070826306913997\n",
      "F1-score: 0.622927180966114\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True)\n",
      "Precisió: 0.6832579185520362\n",
      "Recall: 0.636593591905565\n",
      "F1-score: 0.6591008293321694\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True, use_context_features=True)\n",
      "Precisió: 0.738569753810082\n",
      "Recall: 0.7082630691399663\n",
      "F1-score: 0.7230989956958392\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_esp_io = convert_to_io(train_esp)\n",
    "train_esp_IO = obtenir_token_entity(train_esp_io)\n",
    "\n",
    "testa_esp_io = convert_to_io(testa_esp)\n",
    "testa_esp_real = obtenir_token_entity(testa_esp_io)\n",
    "\n",
    "def model_entrenament(train_tag, extractor, testa_pre_tag):\n",
    "    if extractor == None:\n",
    "        model_IO = CRFTagger()\n",
    "        model_IO.train(train_tag, 'model_IO.crf.tagger')\n",
    "        \n",
    "        predicted_IO = model_IO.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    else:\n",
    "        model_IO = CRFTagger(feature_func=extractor._get_features)\n",
    "        model_IO.train(train_tag, 'model_IO.crf.tagger')\n",
    "        \n",
    "        predicted_IO = model_IO.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    return resultats(predicted_IO, testa_esp_real, obtenir_entitats_amb_posicions_IO)\n",
    "\n",
    "for param in param_combinations_esp: # definida primerament en la predicció del BIO\n",
    "    print(f'Model: {str(param)}') if param != None else print(f'Model: Features per defecte de CRFTagger')\n",
    "    model_entrenament(train_esp_IO, param, testa_esp_pre_tag)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observem que els millors resultats es donen amb el model que usa els 3 'features' diferents, com en el cas anterior. Farem servir el model que utilitza aquestes 3 'features' per a poder identificar les entitats correctament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.779678412589805\n",
      "Recall: 0.7553861451773285\n",
      "F1-score: 0.7673400673400673\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True, model_POS=model_tagger_POS_esp)\n",
    "\n",
    "testb_esp_io = convert_to_io(testb_esp)\n",
    "testb_esp_real = obtenir_token_entity(testb_esp_io)\n",
    "\n",
    "testb_esp_pre_tag = obtenir_token(testb_esp)\n",
    "\n",
    "model_IO_esp = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_IO_esp.train(train_esp_IO, 'model_IO.crf.tagger')\n",
    "    \n",
    "predicted_IO = model_IO_esp.tag_sents(testb_esp_pre_tag)\n",
    "\n",
    "resultats(predicted_IO, testb_esp_real, obtenir_entitats_amb_posicions_IO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentació amb diferents 'features' per a veure quins són els més adequats pel model amb 'IO' i la llengua neerlandesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Features per defecte de CRFTagger\n",
      "Precisió: 0.6347177848775293\n",
      "Recall: 0.5614696184644371\n",
      "F1-score: 0.5958510372406899\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor()\n",
      "Precisió: 0.7369727047146402\n",
      "Recall: 0.13989637305699482\n",
      "F1-score: 0.2351543942992874\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True)\n",
      "Precisió: 0.623082542001461\n",
      "Recall: 0.401789919924635\n",
      "F1-score: 0.4885452462772051\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True)\n",
      "Precisió: 0.6628211851074987\n",
      "Recall: 0.5953838907206783\n",
      "F1-score: 0.6272952853598015\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True, use_context_features=True)\n",
      "Precisió: 0.7160931174089069\n",
      "Recall: 0.6665096561469619\n",
      "F1-score: 0.6904122956818737\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_ned_io = convert_to_io(train_ned)\n",
    "train_ned_IO = obtenir_token_entity(train_ned_io)\n",
    "\n",
    "testa_ned_io = convert_to_io(testa_ned)\n",
    "testa_ned_real = obtenir_token_entity(testa_ned_io)\n",
    "\n",
    "def model_entrenament(train_tag, extractor, testa_pre_tag):\n",
    "    if extractor == None:\n",
    "        model_IO = CRFTagger()\n",
    "        model_IO.train(train_tag, 'model_io_ned.crf.tagger')\n",
    "        \n",
    "        predicted_IO = model_IO.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    else:\n",
    "        model_IO = CRFTagger(feature_func=extractor._get_features)\n",
    "        model_IO.train(train_tag, 'model_io_ned.crf.tagger')\n",
    "        \n",
    "        predicted_IO = model_IO.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    return resultats(predicted_IO, testa_ned_real, obtenir_entitats_amb_posicions_IO)\n",
    "\n",
    "for param in param_combinations_ned: # definida primerament en la predicció del BIO\n",
    "    print(f'Model: {str(param)}') if param != None else print(f'Model: Features per defecte de CRFTagger')\n",
    "    model_entrenament(train_ned_IO, param, testa_ned_pre_tag)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquesta ocasió tornem a observar que els millors resultats es donen amb el model que usa els 3 'features' diferents, per tant, farem servir el model que utilitza aquestes 3 'features' per a poder identificar les entitats correctament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7119216480918609\n",
      "Recall: 0.670057215511761\n",
      "F1-score: 0.6903553299492386\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True, model_POS=model_tagger_POS_ned)\n",
    "\n",
    "testb_ned_io = convert_to_io(testb_ned)\n",
    "testb_ned_real = obtenir_token_entity(testb_ned_io)\n",
    "\n",
    "testb_ned_pre_tag = obtenir_token(testb_ned)\n",
    "\n",
    "model_IO_ned = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_IO_ned.train(train_ned_IO, 'model_io_ned.crf.tagger')\n",
    "    \n",
    "predicted_IO = model_IO_ned.tag_sents(testb_ned_pre_tag)\n",
    "\n",
    "resultats(predicted_IO, testb_ned_real, obtenir_entitats_amb_posicions_IO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue; font-weight:bold;\">Predicció amb BIOES</span> <a id=\"bioes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bioes(train_data_bio):\n",
    "    \"\"\"\n",
    "    Función para convertir datos de formato BIO a formato BIOES.\n",
    "    Argumento:\n",
    "    train_data_bio: una lista de frases, donde cada frase es una lista de tuplas (palabra, etiqueta POS, etiqueta BIO).\n",
    "    Retorna:\n",
    "    Una lista de frases en formato BIOES, donde cada frase es una lista de tuplas (palabra, etiqueta POS, etiqueta BIOES).\n",
    "    \"\"\"\n",
    "    train_data_bioes = []\n",
    "    for sentence in train_data_bio:\n",
    "        bioes_tags = []\n",
    "        for i, (word, pos_tag, bio_tag) in enumerate(sentence):\n",
    "            if bio_tag == 'O':\n",
    "                bioes_tags.append('O')\n",
    "            elif bio_tag.startswith('B-'):\n",
    "                if i == len(sentence) - 1 or sentence[i + 1][2] != 'I' + bio_tag[1:]:\n",
    "                    bioes_tags.append('S' + bio_tag[1:])  # Single\n",
    "                else:\n",
    "                    bioes_tags.append('B' + bio_tag[1:])  # Begin\n",
    "            elif bio_tag.startswith('I-'):\n",
    "                if i == len(sentence) - 1 or sentence[i + 1][2] != 'I' + bio_tag[1:]:\n",
    "                    bioes_tags.append('E' + bio_tag[1:])  # End\n",
    "                else:\n",
    "                    bioes_tags.append('I' + bio_tag[1:])  # Inside\n",
    "            else:\n",
    "                raise ValueError(\"Etiqueta BIO incorrecta: {}\".format(bio_tag))\n",
    "        \n",
    "        train_data_bioes.append([(word, pos_tag, bioes_tag) for (word, pos_tag, bio_tag), bioes_tag in zip(sentence, bioes_tags)])\n",
    "    return train_data_bioes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenir_entitats_amb_posicions_bioes(train_data_bioes):\n",
    "    \"\"\"\n",
    "    Función para agrupar en sets las entidades nombradas, el índice donde empieza y el índice donde acaba,\n",
    "    y la clase de la entidad.\n",
    "    \n",
    "    Argumento: un texto con una tupla (con el token y su entidad) para cada elemento en cada frase.\n",
    "    \"\"\"\n",
    "        \n",
    "    entitats_amb_posicions = set()\n",
    "\n",
    "    for sentence_index, sentence in enumerate(train_data_bioes):\n",
    "        ent = []\n",
    "        name = None\n",
    "        start_pos = None  # Posición de inicio de la entidad actual\n",
    "\n",
    "        for token_index, (word, bioes_tag) in enumerate(sentence):\n",
    "            if bioes_tag != 'O':\n",
    "                if bioes_tag.startswith('B-') or bioes_tag.startswith('S-'):\n",
    "                    # Si hay una entidad anterior, la agregamos a la lista de entidades\n",
    "                    if ent:\n",
    "                        end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                        entitats_amb_posicions.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                    # Creamos una nueva entidad con la palabra actual\n",
    "                    ent = [word]\n",
    "                    # Obtenemos el tipo de entidad\n",
    "                    name = bioes_tag.split('-')[1]\n",
    "                    start_pos = token_index  # La posición de inicio es el token actual\n",
    "                elif bioes_tag.startswith('I-') or bioes_tag.startswith('E-'):\n",
    "                    ent.append(word)\n",
    "            elif ent:\n",
    "                # Si encontramos una etiqueta 'O' y hay una entidad en curso, la agregamos a la lista de entidades\n",
    "                end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                entitats_amb_posicions.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Reiniciamos la lista de la entidad actual\n",
    "                ent = []\n",
    "\n",
    "        # Agregamos la última entidad si la hay\n",
    "        if ent:\n",
    "            end_pos = len(sentence) - 1  # La posición de fin es el último token de la oración\n",
    "            entitats_amb_posicions.add((tuple(ent), (start_pos, end_pos), name))\n",
    "\n",
    "    return entitats_amb_posicions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentació amb diferents 'features' per a veure quins són els més adequats pel model amb 'BIO' i la llengua espanyola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Features per defecte de CRFTagger\n",
      "Precisió: 0.6887340301974448\n",
      "Recall: 0.6565181289786881\n",
      "F1-score: 0.6722403287515941\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor()\n",
      "Precisió: 0.7877838684416602\n",
      "Recall: 0.2784389703847218\n",
      "F1-score: 0.4114519427402863\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True)\n",
      "Precisió: 0.6522241478913923\n",
      "Recall: 0.6249654027124274\n",
      "F1-score: 0.638303886925795\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True)\n",
      "Precisió: 0.7036930178880554\n",
      "Recall: 0.6750622751176307\n",
      "F1-score: 0.6890803785845458\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True, use_context_features=True)\n",
      "Precisió: 0.7442737025224703\n",
      "Recall: 0.7104898975920287\n",
      "F1-score: 0.7269895213820448\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_esp_bioes = convert_to_bioes(train_esp)\n",
    "train_esp_BIOES = obtenir_token_entity(train_esp_bioes)\n",
    "\n",
    "testa_esp_bioes = convert_to_bioes(testa_esp)\n",
    "testa_esp_real = obtenir_token_entity(testa_esp_bioes)\n",
    "\n",
    "def model_entrenament(train_tag, extractor, testa_pre_tag):\n",
    "    if extractor == None:\n",
    "        model_BIOES = CRFTagger()\n",
    "        model_BIOES.train(train_tag, 'model_BIOES_esp.crf.tagger')\n",
    "        \n",
    "        predicted_BIOES = model_BIOES.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    else:\n",
    "        model_BIOES = CRFTagger(feature_func=extractor._get_features)\n",
    "        model_BIOES.train(train_tag, 'model_BIOES_esp.crf.tagger')\n",
    "        \n",
    "        predicted_BIOES = model_BIOES.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    return resultats(predicted_BIOES, testa_esp_real, obtenir_entitats_amb_posicions_bioes)\n",
    "\n",
    "for param in param_combinations_esp: # definida primerament en la predicció del BIO\n",
    "    print(f'Model: {str(param)}') if param != None else print(f'Model: Features per defecte de CRFTagger')\n",
    "    model_entrenament(train_esp_BIOES, param, testa_esp_pre_tag)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tornem a veure com el model que usa el 3 'features' obte els millors resultats i en aquesta ocasió tornarem a utilitzar el model que utilitza els 3 'features' per a identificar les entitats correctament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7817327065144393\n",
      "Recall: 0.7673038892551087\n",
      "F1-score: 0.7744510978043911\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True, model_POS=model_tagger_POS_esp)\n",
    "\n",
    "testb_esp_bioes = convert_to_bioes(testb_esp)\n",
    "testb_esp_real = obtenir_token_entity(testb_esp_bioes)\n",
    "\n",
    "testb_esp_pre_tag = obtenir_token(testb_esp)\n",
    "\n",
    "model_BIOES_esp = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIOES_esp.train(train_esp_BIOES, 'model_BIOES_esp.crf.tagger')\n",
    "    \n",
    "predicted_BIOES = model_BIOES_esp.tag_sents(testb_esp_pre_tag)\n",
    "\n",
    "resultats(predicted_BIOES, testb_esp_real, obtenir_entitats_amb_posicions_bioes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentació amb diferents 'features' per a veure quins són els més adequats pel model amb 'BIOES' i la llengua neerlandesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Features per defecte de CRFTagger\n",
      "Precisió: 0.0\n",
      "Recall: 0.0\n",
      "F1-score: 0.0\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor()\n",
      "Precisió: 0.7785087719298246\n",
      "Recall: 0.16202647193062528\n",
      "F1-score: 0.26822818284850775\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True)\n",
      "Precisió: 0.6494252873563219\n",
      "Recall: 0.41259698767685987\n",
      "F1-score: 0.504605079542283\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True)\n",
      "Precisió: 0.6781725888324873\n",
      "Recall: 0.6097672295755363\n",
      "F1-score: 0.6421533285267964\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True, use_context_features=True)\n",
      "Precisió: 0.7135802469135802\n",
      "Recall: 0.659516202647193\n",
      "F1-score: 0.6854838709677419\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_ned_bioes = convert_to_bioes(train_ned)\n",
    "train_ned_BIOES = obtenir_token_entity(train_ned_bioes)\n",
    "\n",
    "testa_ned_bioes = convert_to_bioes(testa_ned)\n",
    "testa_ned_real = obtenir_token_entity(testa_ned_bioes)\n",
    "\n",
    "def model_entrenament(train_tag, extractor, testa_pre_tag):\n",
    "    if extractor == None:\n",
    "        model_BIOES = CRFTagger()\n",
    "        model_BIOES.train(train_tag, 'model_BIOES_ned.crf.tagger')\n",
    "        \n",
    "        predicted_BIOES = model_BIOES.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    else:\n",
    "        model_BIOES = CRFTagger(feature_func=extractor._get_features)\n",
    "        model_BIOES.train(train_tag, 'model_BIOES_ned.crf.tagger')\n",
    "        \n",
    "        predicted_BIOES = model_BIOES.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    return resultats(predicted_BIOES, testa_ned_real, obtenir_entitats_amb_posicions_bioes)\n",
    "\n",
    "for param in param_combinations_ned: # definida primerament en la predicció del BIO\n",
    "    print(f'Model: {str(param)}') if param != None else print(f'Model: Features per defecte de CRFTagger')\n",
    "    model_entrenament(train_ned_BIOES, param, testa_ned_pre_tag)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest últim model veiem com una vegada més els millors resultats ens el dona el model que usa els 3 'features' i, per tant, una vegada més utilitzarem aquest model per identificar correctament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7451505016722408\n",
      "Recall: 0.6853275915103045\n",
      "F1-score: 0.7139881429258131\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True, model_POS=model_tagger_POS_ned)\n",
    "\n",
    "testb_ned_bioes = convert_to_bioes(testb_ned)\n",
    "testb_ned_real = obtenir_token_entity(testb_ned_bioes)\n",
    "\n",
    "testb_ned_pre_tag = obtenir_token(testb_ned)\n",
    "\n",
    "model_BIOES_ned = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIOES_ned.train(train_ned_BIOES, 'model_BIOES_ned.crf.tagger')\n",
    "    \n",
    "predicted_BIOES = model_BIOES_ned.tag_sents(testb_ned_pre_tag)\n",
    "\n",
    "resultats(predicted_BIOES, testb_ned_real, obtenir_entitats_amb_posicions_bioes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue; font-weight:bold;\">CONCLUSIONS</span> <a id=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Després de les proves realitzades, es pot observar clarament que l'ús de múltiples 'features' en un model millora significativament la identificació d'entitats. Aquesta millora és notable en comparació amb models que utilitzen menys característiques o cap, destacant la importància d'aquesta pràctica en el desenvolupament de models precisos.\n",
    "\n",
    "Pel que fa a les codificacions BIO, IO i BIOES en les tasques d'etiquetatge d'entitats anomenades, les proves mostren que proporcionen resultats molt similars, sense destacar notablement l'una sobre les altres. Tot i això, creiem que és important assenyalar que la codificació IO tendeix a obtenir lleugerament resultats pitjors en comparació amb les altres dues, tot i que aquesta diferència no és significativa o notòria. Aquest fenomen pot atribuir-se a la menor quantitat d'informació proporcionada per la codificació IO en la detecció d'entitats, limitant així les opcions dels models predictius en la seva capacitat per detectar-les o no.\n",
    "\n",
    "Quant a la comparació entre els models en castellà i neerlandès, és evident que els models en castellà mostren millors resultats. Aquesta diferència pot atribuir-se a diversos factors, com ara la disponibilitat de dades etiquetades i corpus de text, la complexitat lingüística i els esforços de recerca i desenvolupament en cada idioma.\n",
    "\n",
    "En resum, és important l'ús de múltiples 'features' en els models, les codificacions 'BIO', 'IO' i 'BIOES' donen resultats molt semblants. Tot i així, es pot observar un rendiment lleugerament inferior amb la codificació 'IO' en comparació amb les altres. A més, els models entrenats en castellà demostren un millor rendiment en comparació amb els models en neerlandès."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execució dels models amb textos reals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

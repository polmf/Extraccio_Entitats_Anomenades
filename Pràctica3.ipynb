{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:lightblue; font-weight:bold;\">EXTRACCIÓ D'ENTITATS ANOMENADES</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ADD8E6; font-weight:bold;\">Índex:</span>\n",
    "### <span style=\"color:#FFFFFF;\">1. [Predicció amb BIO](#BIO)</span>\n",
    "### <span style=\"color:#FFFFFF;\">2. [Predicció amb IO](#IO)</span>\n",
    "### <span style=\"color:#ADD8E6;\">3. [Predicció amb BIOES](#BIOES)</span>\n",
    "### <span style=\"color:#ADD8E6;\">4. [Conclusions](#conclusions)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue;\"> Imports </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pycrfsuite\n",
    "from nltk.corpus import conll2002\n",
    "from nltk.tag import CRFTagger\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt') # Tokenitzador\n",
    "nltk.download('averaged_perceptron_tagger') # Etiquetador POS\n",
    "nltk.download('maxent_ne_chunker') # Etiquetador Entitats Anomenades\n",
    "nltk.download('words')\n",
    "nltk.download('treebank')\n",
    "nltk.download('conll2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_esp = conll2002.iob_sents('esp.train') # Train\n",
    "testa_esp = conll2002.iob_sents('esp.testa') # Dev\n",
    "testb_esp = conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "testa_ned = conll2002.iob_sents('ned.testa') # Dev\n",
    "testb_ned = conll2002.iob_sents('ned.testb') # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_token_POS(fitxer):\n",
    "    \"\"\"\n",
    "    Funció per convertir un text amb el token, POS i entitat per \n",
    "    cada element en cada frase en un text amb el token i el seu POS\n",
    "    per cada element.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for sentence in fitxer:\n",
    "        frases = []\n",
    "        for elem1, elem2, elem3 in sentence:\n",
    "            frases.append((elem1, elem2))\n",
    "        res.append(frases)\n",
    "    return res\n",
    "\n",
    "# Fem prediccions i mirem l'accuracy\n",
    "def obtener_token(fitxer):\n",
    "    \"\"\"\n",
    "    Funció per convertir un text amb el token, POS i entitat \n",
    "    per cada element en cada frase en un text amb només el token\n",
    "    per cada element.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for sentence in fitxer:\n",
    "        frases = []\n",
    "        for elem1, elem2, elem3 in sentence:\n",
    "            frases.append(elem1)\n",
    "        res.append(frases)\n",
    "    return res\n",
    "\n",
    "\n",
    "def obtener_token_entity(fitxer):\n",
    "    \"\"\"\n",
    "    Funció per convertir un text amb el token, POS i entitat per cada \n",
    "    element en cada frase en un text amb el token i la seva entitat per \n",
    "    cada element.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for sentence in fitxer:\n",
    "        frases = []\n",
    "        for elem1, elem2, elem3 in sentence:\n",
    "            frases.append((elem1, elem3))\n",
    "        res.append(frases)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue;\"> Classe FeatureExtractor personalitzada </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span> Features que es tenen en compte:\n",
    "<ul>\n",
    "    <li>Paraula actual</li>\n",
    "    <li>Si comença en majúscula</li>\n",
    "    <li>Si té signe de puntuació</li>\n",
    "    <li>Si té números</li>\n",
    "    <li>Prefixos fins a longitud 3</li>\n",
    "    <li>Sufixos fins a longitud 3</li>\n",
    "    <li>Paraules prèvies i posteriors amb POS</li>\n",
    "    <li>POS-tags</li>\n",
    "    <li>Longitud de la paraula</li>\n",
    "</ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \n",
    "    \"\"\"\n",
    "    Aquesta classe conté el mètode per calcular les features\n",
    "    que s'utilitzaran per entrenar el model més endavant.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_basic_features=False, use_prefix_suffix_features=False, use_context_features=False, pattern = r'\\d+', model_POS = None):\n",
    "        self.use_basic_features = use_basic_features\n",
    "        self.use_prefix_suffix_features = use_prefix_suffix_features\n",
    "        self.use_context_features = use_context_features\n",
    "        self._pattern = pattern\n",
    "        self.model_POS = model_POS\n",
    "        \n",
    "    def __str__(self):\n",
    "        features = []\n",
    "        if self.use_basic_features:\n",
    "            features.append(\"use_basic_features=True\")\n",
    "        if self.use_prefix_suffix_features:\n",
    "            features.append(\"use_prefix_suffix_features=True\")\n",
    "        if self.use_context_features:\n",
    "            features.append(\"use_context_features=True\")\n",
    "        return f\"FeatureExtractor({', '.join(features)})\"\n",
    "        \n",
    "    def _get_features(self, tokens, idx):\n",
    "        \"\"\"\n",
    "        Extract basic features about this word including\n",
    "            - Current word\n",
    "            - is it capitalized?\n",
    "            - Does it have punctuation?\n",
    "            - Does it have a number?\n",
    "            - Preffixes up to length 3\n",
    "            - Suffixes up to length 3\n",
    "            - paraules prèvies i posteriors amb POS\n",
    "            - POS-tags\n",
    "            - longitud\n",
    "\n",
    "        Note that : we might include feature over previous word, next word etc.\n",
    "\n",
    "        :return: a list which contains the features\n",
    "        :rtype: list(str)\n",
    "        \"\"\"\n",
    "            \n",
    "        token = tokens[idx]\n",
    "        \n",
    "        feature_list = []\n",
    "        \n",
    "        if self.use_basic_features:\n",
    "            # Capitalization\n",
    "            if token[0].isupper():\n",
    "                feature_list.append(\"CAPITALIZATION\")\n",
    "\n",
    "            # Number\n",
    "            if re.search(self._pattern, token) is not None:\n",
    "                feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "            # Punctuation\n",
    "            punc_cat = {\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"}\n",
    "            if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "                feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "                    \n",
    "        if self.use_prefix_suffix_features:\n",
    "            # preffix up to length 3\n",
    "            if len(token) > 1:\n",
    "                feature_list.append(\"PRE_\" + token[:1])\n",
    "            if len(token) > 2:\n",
    "                feature_list.append(\"PRE_\" + token[:2])\n",
    "            if len(token) > 3:\n",
    "                feature_list.append(\"PRE_\" + token[:3])\n",
    "\n",
    "            # Suffix up to length 3\n",
    "            if len(token) > 1:\n",
    "                feature_list.append(\"SUF_\" + token[-1:])\n",
    "            if len(token) > 2:\n",
    "                feature_list.append(\"SUF_\" + token[-2:])\n",
    "            if len(token) > 3:\n",
    "                feature_list.append(\"SUF_\" + token[-3:])\n",
    "    \n",
    "        \n",
    "        if self.use_context_features:\n",
    "            # POS_tags\n",
    "            POS = self.model_POS.tag(tokens)\n",
    "                \n",
    "            # Paraules prèvies amb POS\n",
    "            if idx > 0:\n",
    "                feature_list.append(\"anterior1_\" + tokens[idx-1] + \"_\" + POS[idx-1][1])\n",
    "            if idx > 1:\n",
    "                feature_list.append(\"anterior2_\" + tokens[idx-2] + \"_\" + POS[idx-2][1])\n",
    "                \n",
    "            # Paraules posteriors amb POS\n",
    "            if idx < (len(tokens)-1):\n",
    "                feature_list.append(\"posterior1_\" + tokens[idx+1] + \"_\" + POS[idx+1][1])\n",
    "            if idx < (len(tokens)-2):\n",
    "                feature_list.append(\"posterior2_\" + tokens[idx+2] + \"_\" + POS[idx+2][1])\n",
    "\n",
    "            feature_list.append(\"WORD_\" + token + \"_\" + POS[idx][1])\n",
    "            \n",
    "        \n",
    "        if not self.use_context_features:\n",
    "            feature_list.append(\"WORD_\" + token)\n",
    "            \n",
    "        \n",
    "        return feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue;\"> Mètriques d'avaluació </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_precision(entidades_referencia, entidades_predichas):\n",
    "    # Calcular el número de entidades correctamente extraídas\n",
    "    entidades_correctas = entidades_referencia.intersection(entidades_predichas)\n",
    "    \n",
    "    # Calcular la precisión\n",
    "    if len(entidades_predichas) > 0:\n",
    "        precision = len(entidades_correctas) / len(entidades_predichas)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    return precision\n",
    "\n",
    "\n",
    "def calcular_recall(entidades_referencia, entidades_predichas):\n",
    "    # Calcular el número de entidades correctamente extraídas\n",
    "    entidades_correctas = entidades_referencia.intersection(entidades_predichas)\n",
    "    \n",
    "    # Calcular la exhaustividad\n",
    "    if len(entidades_referencia) > 0:\n",
    "        exhaustividad = len(entidades_correctas) / len(entidades_referencia)\n",
    "    else:\n",
    "        exhaustividad = 0.0\n",
    "    \n",
    "    return exhaustividad\n",
    "\n",
    "def calcular_f1_score(precision, recall):\n",
    "    # Calcular el F1-score\n",
    "    if (precision + recall) > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "def resultats(predicted_BIO, testa_esp_BIO_tag, obtain_entity):\n",
    "\n",
    "    # Obtener los conjuntos de entidades de referencia y extraídas\n",
    "    entidades_referencia = obtain_entity(testa_esp_BIO_tag)  # Conjunto de entidades etiquetadas manualmente como referencia\n",
    "    entidades_predichas =  obtain_entity(predicted_BIO) # Obtener conjuntos de entidades predichas\n",
    "\n",
    "    # Calcular la precisión\n",
    "    precision = calcular_precision(entidades_referencia, entidades_predichas)\n",
    "\n",
    "    # Calcular la exhaustividad\n",
    "    recall = calcular_recall(entidades_referencia, entidades_predichas)\n",
    "\n",
    "    # Calcular el F1-score\n",
    "    f1_score = calcular_f1_score(precision, recall)\n",
    "\n",
    "    print(\"Precisió:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue; font-weight:bold;\">Predicció amb BIO</span> <a id=\"BIO\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaluació amb sets veure si son iguals també es pot mirar per intersecció i si coincideixen\n",
    "def obtener_entidades_con_posiciones_BIO(fitxer_BIO_tag):\n",
    "    \"\"\"\n",
    "    Funció per agrupar en sets les entitats anomenades, l'índex on comença i l'índex on acaba,\n",
    "    i la classe de la entitat.\n",
    "    \n",
    "    Argument: un text amb una tupla (amb el token i la seva entitat) per cada element en cada frase.\n",
    "    \"\"\"\n",
    "    \n",
    "    entitats_con_posiciones = set()\n",
    "\n",
    "    for sentence_index, sentence in enumerate(fitxer_BIO_tag):\n",
    "        ent = []\n",
    "        name = None\n",
    "        start_pos = None  # Posición de inicio de la entidad actual\n",
    "        prev_tag = None  # Almacenar la etiqueta del token anterior\n",
    "\n",
    "        for token_index, token in enumerate(sentence):\n",
    "            word, tag = token\n",
    "            #word = word[0]\n",
    "\n",
    "            if tag.startswith('B-'):\n",
    "                # Si hay una entidad anterior, la agregamos a la lista de entidades\n",
    "                if ent:\n",
    "                    end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                    entitats_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Creamos una nueva entidad con la palabra actual\n",
    "                ent = [word]\n",
    "                # Obtenemos el tipo de entidad\n",
    "                name = tag.split('-')[1]\n",
    "                start_pos = token_index  # La posición de inicio es el token actual\n",
    "                prev_tag = tag  # Actualizamos la etiqueta del token anterior\n",
    "            elif tag.startswith('I-'):\n",
    "                # Solo agregamos la palabra actual si el token anterior tiene etiqueta I- o B-\n",
    "                if prev_tag:\n",
    "                    ent.append(word)\n",
    "                    prev_tag = tag  # Actualizamos la etiqueta del token anterior\n",
    "            elif tag == 'O' and ent:\n",
    "                # Si encontramos una etiqueta 'O' y hay una entidad en curso, la agregamos a la lista de entidades\n",
    "                end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                entitats_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Reiniciamos la lista de la entidad actual\n",
    "                ent = []\n",
    "                prev_tag = None  # Reiniciamos la etiqueta del token anterior\n",
    "\n",
    "        # Agregamos la última entidad si la hay\n",
    "        if ent:\n",
    "            end_pos = len(sentence) - 1  # La posición de fin es el último token de la oración\n",
    "            entitats_con_posiciones.add((tuple(ent), (start_pos, end_pos)))\n",
    "\n",
    "    return entitats_con_posiciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9447121289420479\n"
     ]
    }
   ],
   "source": [
    "model_tagger_POS_esp = CRFTagger()\n",
    "\n",
    "# Entrenem el model per predir els POS que corresponen a cada token\n",
    "\n",
    "train_esp_pos_tag = obtener_token_POS(train_esp)\n",
    "    \n",
    "model_tagger_POS_esp.train(train_esp_pos_tag, 'model_POS_esp.crf.tagger')    \n",
    "\n",
    "\n",
    "testa_esp_pre_tag = obtener_token(testa_esp)\n",
    "    \n",
    "predicted = model_tagger_POS_esp.tag_sents(testa_esp_pre_tag)\n",
    "\n",
    "predictions = [elem[1] for sentence in predicted for elem in sentence]\n",
    "real_label = [elem[1] for sentence in testa_esp for elem in sentence]\n",
    "\n",
    "print(accuracy_score(predictions, real_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: None\n",
      "Precisió: 0.6927890345649583\n",
      "Recall: 0.643687707641196\n",
      "F1-score: 0.6673363949483353\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor()\n",
      "Precisió: 0.718299164768413\n",
      "Recall: 0.2619047619047619\n",
      "F1-score: 0.3838506796510448\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor(use_basic_features=True)\n",
      "Precisió: 0.6447638603696099\n",
      "Recall: 0.6085271317829457\n",
      "F1-score: 0.626121635094716\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True)\n",
      "Precisió: 0.7015675835551612\n",
      "Recall: 0.6566998892580288\n",
      "F1-score: 0.6783926783926784\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True, use_context_features=True)\n",
      "Precisió: 0.7475160724722385\n",
      "Recall: 0.7081949058693244\n",
      "F1-score: 0.7273244242251918\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "param_combinations_esp = [\n",
    "    None,\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True, model_POS=model_tagger_POS_esp),\n",
    "    FeatureExtractor(True, True, model_POS=model_tagger_POS_esp),\n",
    "    FeatureExtractor(True, True, True, model_POS=model_tagger_POS_esp)\n",
    "]\n",
    "\n",
    "train_esp_BIO = obtener_token_entity(train_esp)\n",
    "\n",
    "testa_esp_real = obtener_token_entity(testa_esp)\n",
    "\n",
    "def model_entrenament(train_esp_BIO_tag, extractor, testa_esp_pre_tag):\n",
    "    if extractor == None:\n",
    "        model_BIO = CRFTagger()\n",
    "        model_BIO.train(train_esp_BIO_tag, 'model_BIO.crf.tagger')\n",
    "        \n",
    "        predicted_BIO = model_BIO.tag_sents(testa_esp_pre_tag)\n",
    "    \n",
    "    else:\n",
    "        model_BIO = CRFTagger(feature_func=extractor._get_features)\n",
    "        model_BIO.train(train_esp_BIO_tag, 'model_BIO.crf.tagger')\n",
    "        \n",
    "        predicted_BIO = model_BIO.tag_sents(testa_esp_pre_tag)\n",
    "    \n",
    "    return resultats(predicted_BIO, testa_esp_real, obtener_entidades_con_posiciones_BIO)\n",
    "\n",
    "for param in param_combinations_esp:\n",
    "    print(f'Modelo: {str(param)}') if param != None else print(f'Modelo: Features per defecte de CRFTagger')\n",
    "    model_entrenament(train_esp_BIO, param, testa_esp_pre_tag)\n",
    "    print('-'*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7869019341703427\n",
      "Recall: 0.7645895153313551\n",
      "F1-score: 0.7755852842809364\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True, model_POS=model_tagger_POS_esp)\n",
    "\n",
    "testb_esp_real = obtener_token_entity(testb_esp)\n",
    "testb_esp_pre_tag = obtener_token(testb_esp)\n",
    "\n",
    "model_BIO = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIO.train(train_esp_BIO, 'model_BIO.crf.tagger')\n",
    "    \n",
    "predicted_BIO = model_BIO.tag_sents(testb_esp_pre_tag)\n",
    "\n",
    "resultats(predicted_BIO, testb_esp_real, obtener_entidades_con_posiciones_BIO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.940191577997718\n"
     ]
    }
   ],
   "source": [
    "model_tagger_POS_ned = CRFTagger()\n",
    "\n",
    "# Entrenem el model per predir els POS que corresponen a cada token\n",
    "train_ned_pos_tag = obtener_token_POS(train_ned)\n",
    "    \n",
    "model_tagger_POS_ned.train(train_ned_pos_tag, 'model_POS_ned.crf.tagger')\n",
    "\n",
    "\n",
    "# Fem prediccions i mirem l'accuracy\n",
    "testa_ned_pre_tag = obtener_token(testa_ned)\n",
    "    \n",
    "predicted = model_tagger_POS_ned.tag_sents(testa_ned_pre_tag)\n",
    "\n",
    "predictions = [elem[1] for sentence in predicted for elem in sentence]\n",
    "real_label = [elem[1] for sentence in testa_ned for elem in sentence]\n",
    "\n",
    "print(accuracy_score(predictions, real_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: None\n",
      "Precisió: 0.6441908713692946\n",
      "Recall: 0.565059144676979\n",
      "F1-score: 0.602035870092099\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor()\n",
      "Precisió: 0.762114537444934\n",
      "Recall: 0.15741583257506825\n",
      "F1-score: 0.2609351432880845\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor(use_basic_features=True)\n",
      "Precisió: 0.655096011816839\n",
      "Recall: 0.4035486806187443\n",
      "F1-score: 0.49943693693693697\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True)\n",
      "Precisió: 0.6853182751540041\n",
      "Recall: 0.6073703366696998\n",
      "F1-score: 0.6439942112879885\n",
      "--------------------------------------------------\n",
      "Model: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True, use_context_features=True)\n",
      "Precisió: 0.7082306554953179\n",
      "Recall: 0.6537761601455869\n",
      "F1-score: 0.6799148332150462\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "param_combinations_ned = [\n",
    "    None,\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True, model_POS=model_tagger_POS_ned),\n",
    "    FeatureExtractor(True, True, model_POS=model_tagger_POS_ned),\n",
    "    FeatureExtractor(True, True, True, model_POS=model_tagger_POS_ned)\n",
    "]\n",
    "\n",
    "train_ned_BIO = obtener_token_entity(train_ned)\n",
    "\n",
    "testa_ned_real = obtener_token_entity(testa_ned)\n",
    "\n",
    "def model_entrenament(train_BIO_tag, extractor, testa_pre_tag):\n",
    "    if extractor == None:\n",
    "        model_BIO = CRFTagger()\n",
    "        model_BIO.train(train_BIO_tag, 'model_BIO_ned.crf.tagger')\n",
    "        \n",
    "        predicted_BIO = model_BIO.tag_sents(testa_pre_tag)\n",
    "    \n",
    "    else:    \n",
    "        model_BIO = CRFTagger(feature_func=extractor._get_features)\n",
    "        model_BIO.train(train_BIO_tag, 'model_BIO_ned.crf.tagger')\n",
    "        \n",
    "        predicted_BIO = model_BIO.tag_sents(testa_pre_tag)\n",
    "    \n",
    "    return resultats(predicted_BIO, testa_ned_real, obtener_entidades_con_posiciones_BIO)\n",
    "\n",
    "for param in param_combinations_ned:\n",
    "    print(f'Modelo: {str(param)}') if param != None else print(f'Modelo: Features per defecte de CRFTagger')\n",
    "    model_entrenament(train_ned_BIO, param, testa_ned_pre_tag)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7402511566424322\n",
      "Recall: 0.6860643185298622\n",
      "F1-score: 0.7121284374503258\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True, model_POS=model_tagger_POS_ned)\n",
    "\n",
    "testb_ned_real = obtener_token_entity(testb_ned)\n",
    "testb_ned_pre_tag = obtener_token(testb_ned)\n",
    "\n",
    "model_BIO = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIO.train(train_ned_BIO, 'model_BIO_ned.crf.tagger')\n",
    "    \n",
    "predicted_BIO = model_BIO.tag_sents(testb_ned_pre_tag)\n",
    "\n",
    "resultats(predicted_BIO, testb_ned_real, obtener_entidades_con_posiciones_BIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue; font-weight:bold;\">Predicció amb IO</span> <a id=\"IO\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_io(train_data_bio):\n",
    "    train_data_io = []\n",
    "    for sentence in train_data_bio:\n",
    "        io_tags = []\n",
    "        for word, pos_tag, bio_tag in sentence:\n",
    "            if bio_tag == 'O':\n",
    "                io_tags.append('O')\n",
    "            elif bio_tag.startswith('B-'):\n",
    "                io_tags.append('I' + bio_tag[1:])\n",
    "            else:\n",
    "                io_tags.append(bio_tag)\n",
    "        \n",
    "        train_data_io.append([(word, pos_tag, io_tag) for (word, pos_tag, bio_tag), io_tag in zip(sentence, io_tags)])\n",
    "    return train_data_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_entidades_con_posiciones_IO(fitxer_IO_tag):\n",
    "    \"\"\"\n",
    "    Función para agrupar en sets las entidades nombradas, el índice donde empieza y el índice donde acaba,\n",
    "    y la clase de la entidad.\n",
    "    \n",
    "    Argumento: un texto con una tupla (con el token y su entidad) para cada elemento en cada frase.\n",
    "    \"\"\"\n",
    "    \n",
    "    entitats_con_posiciones = set()\n",
    "\n",
    "    for sentence_index, sentence in enumerate(fitxer_IO_tag):\n",
    "        ent = []\n",
    "        name = None\n",
    "        start_pos = None  # Posición de inicio de la entidad actual\n",
    "        prev_tag = None  # Almacenar la etiqueta del token anterior\n",
    "\n",
    "        for token_index, token in enumerate(sentence):\n",
    "            word, tag = token\n",
    "\n",
    "            if tag.startswith('I-'):\n",
    "                # Si la etiqueta es 'I-' y el token anterior es 'O', comenzamos una nueva entidad\n",
    "                if not prev_tag:\n",
    "                    # Si hay una entidad anterior, la agregamos a la lista de entidades\n",
    "                    if ent:\n",
    "                        end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                        entitats_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                    # Creamos una nueva entidad con la palabra actual\n",
    "                    ent = [word]\n",
    "                    # Obtenemos el tipo de entidad\n",
    "                    name = tag.split('-')[1]\n",
    "                    start_pos = token_index  # La posición de inicio es el token actual\n",
    "                else:\n",
    "                    # Si el token anterior es 'I-', agregamos la palabra actual a la entidad en curso\n",
    "                    ent.append(word)\n",
    "                prev_tag = tag  # Actualizamos la etiqueta del token anterior\n",
    "            \n",
    "            elif tag == 'O' and ent:\n",
    "                # Si encontramos una etiqueta 'O' y hay una entidad en curso, la agregamos a la lista de entidades\n",
    "                end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                entitats_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Reiniciamos la lista de la entidad actual\n",
    "                ent = []\n",
    "                prev_tag = None  # Reiniciamos la etiqueta del token anterior\n",
    "\n",
    "        # Agregamos la última entidad si la hay\n",
    "        if ent:\n",
    "            end_pos = len(sentence) - 1  # La posición de fin es el último token de la oración\n",
    "            entitats_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "\n",
    "    return entitats_con_posiciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: None\n",
      "Precisió: 0.6720413751140858\n",
      "Recall: 0.6208544125913434\n",
      "F1-score: 0.6454346238130021\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor()\n",
      "Precisió: 0.6519083969465649\n",
      "Recall: 0.24002248454187747\n",
      "F1-score: 0.35086277732128185\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True)\n",
      "Precisió: 0.6396209653538644\n",
      "Recall: 0.6070826306913997\n",
      "F1-score: 0.622927180966114\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True)\n",
      "Precisió: 0.6832579185520362\n",
      "Recall: 0.636593591905565\n",
      "F1-score: 0.6591008293321694\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True, use_context_features=True)\n",
      "Precisió: 0.738569753810082\n",
      "Recall: 0.7082630691399663\n",
      "F1-score: 0.7230989956958392\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_esp_io = convert_to_io(train_esp)\n",
    "train_esp_IO = obtener_token_entity(train_esp_io)\n",
    "\n",
    "testa_esp_io = convert_to_io(testa_esp)\n",
    "testa_esp_real = obtener_token_entity(testa_esp_io)\n",
    "\n",
    "def model_entrenament(train_tag, extractor, testa_pre_tag):\n",
    "    if extractor == None:\n",
    "        model_IO = CRFTagger()\n",
    "        model_IO.train(train_tag, 'model_IO.crf.tagger')\n",
    "        \n",
    "        predicted_IO = model_IO.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    else:\n",
    "        model_IO = CRFTagger(feature_func=extractor._get_features)\n",
    "        model_IO.train(train_tag, 'model_IO.crf.tagger')\n",
    "        \n",
    "        predicted_IO = model_IO.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    return resultats(predicted_IO, testa_esp_real, obtener_entidades_con_posiciones_IO)\n",
    "\n",
    "for param in param_combinations_esp: # definida primerament en la predicció del BIO\n",
    "    print(f'Modelo: {str(param)}') if param != None else print(f'Modelo: Features per defecte de CRFTagger')\n",
    "    model_entrenament(train_esp_IO, param, testa_esp_pre_tag)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.779678412589805\n",
      "Recall: 0.7553861451773285\n",
      "F1-score: 0.7673400673400673\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True, model_POS=model_tagger_POS_esp)\n",
    "\n",
    "testb_esp_io = convert_to_io(testb_esp)\n",
    "testb_esp_real = obtener_token_entity(testb_esp_io)\n",
    "\n",
    "testb_esp_pre_tag = obtener_token(testb_esp)\n",
    "\n",
    "model_IO = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_IO.train(train_esp_IO, 'model_IO.crf.tagger')\n",
    "    \n",
    "predicted_IO = model_IO.tag_sents(testb_esp_pre_tag)\n",
    "\n",
    "resultats(predicted_IO, testb_esp_real, obtener_entidades_con_posiciones_IO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Features per defecte de CRFTagger\n",
      "Precisió: 0.6347177848775293\n",
      "Recall: 0.5614696184644371\n",
      "F1-score: 0.5958510372406899\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor()\n",
      "Precisió: 0.7369727047146402\n",
      "Recall: 0.13989637305699482\n",
      "F1-score: 0.2351543942992874\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True)\n",
      "Precisió: 0.623082542001461\n",
      "Recall: 0.401789919924635\n",
      "F1-score: 0.4885452462772051\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True)\n",
      "Precisió: 0.6628211851074987\n",
      "Recall: 0.5953838907206783\n",
      "F1-score: 0.6272952853598015\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True, use_context_features=True)\n",
      "Precisió: 0.7160931174089069\n",
      "Recall: 0.6665096561469619\n",
      "F1-score: 0.6904122956818737\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_ned_io = convert_to_io(train_ned)\n",
    "train_ned_IO = obtener_token_entity(train_ned_io)\n",
    "\n",
    "testa_ned_io = convert_to_io(testa_ned)\n",
    "testa_ned_real = obtener_token_entity(testa_ned_io)\n",
    "\n",
    "def model_entrenament(train_tag, extractor, testa_pre_tag):\n",
    "    if extractor == None:\n",
    "        model_IO = CRFTagger()\n",
    "        model_IO.train(train_tag, 'model_io_ned.crf.tagger')\n",
    "        \n",
    "        predicted_IO = model_IO.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    else:\n",
    "        model_IO = CRFTagger(feature_func=extractor._get_features)\n",
    "        model_IO.train(train_tag, 'model_io_ned.crf.tagger')\n",
    "        \n",
    "        predicted_IO = model_IO.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    return resultats(predicted_IO, testa_ned_real, obtener_entidades_con_posiciones_IO)\n",
    "\n",
    "for param in param_combinations_ned: # definida primerament en la predicció del BIO\n",
    "    print(f'Modelo: {str(param)}') if param != None else print(f'Modelo: Features per defecte de CRFTagger')\n",
    "    model_entrenament(train_ned_IO, param, testa_ned_pre_tag)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7119216480918609\n",
      "Recall: 0.670057215511761\n",
      "F1-score: 0.6903553299492386\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True, model_POS=model_tagger_POS_ned)\n",
    "\n",
    "testb_ned_io = convert_to_io(testb_ned)\n",
    "testb_ned_real = obtener_token_entity(testb_ned_io)\n",
    "\n",
    "testb_ned_pre_tag = obtener_token(testb_ned)\n",
    "\n",
    "model_IO = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_IO.train(train_ned_IO, 'model_io_ned.crf.tagger')\n",
    "    \n",
    "predicted_IO = model_IO.tag_sents(testb_ned_pre_tag)\n",
    "\n",
    "resultats(predicted_IO, testb_ned_real, obtener_entidades_con_posiciones_IO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue; font-weight:bold;\">Predicció amb BIOES</span> <a id=\"BIOES\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bioes(train_data_bio):\n",
    "    train_data_bioes = []\n",
    "    for sentence in train_data_bio:\n",
    "        bioes_tags = []\n",
    "        for i, (word, pos_tag, bio_tag) in enumerate(sentence):\n",
    "            if bio_tag == 'O':\n",
    "                bioes_tags.append('O')\n",
    "            elif bio_tag.startswith('B-'):\n",
    "                if i == len(sentence) - 1 or sentence[i + 1][2] != 'I' + bio_tag[1:]:\n",
    "                    bioes_tags.append('S' + bio_tag[1:])  # Single\n",
    "                else:\n",
    "                    bioes_tags.append('B' + bio_tag[1:])  # Begin\n",
    "            elif bio_tag.startswith('I-'):\n",
    "                if i == len(sentence) - 1 or sentence[i + 1][2] != 'I' + bio_tag[1:]:\n",
    "                    bioes_tags.append('E' + bio_tag[1:])  # End\n",
    "                else:\n",
    "                    bioes_tags.append('I' + bio_tag[1:])  # Inside\n",
    "            else:\n",
    "                raise ValueError(\"Etiqueta BIO incorrecta: {}\".format(bio_tag))\n",
    "        \n",
    "        train_data_bioes.append([(word, pos_tag, bioes_tag) for (word, pos_tag, bio_tag), bioes_tag in zip(sentence, bioes_tags)])\n",
    "    return train_data_bioes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_entidades_con_posiciones_bioes(train_data_bioes):\n",
    "    \"\"\"\n",
    "    Función para agrupar en sets las entidades nombradas, el índice donde empieza y el índice donde acaba,\n",
    "    y la clase de la entidad.\n",
    "    \n",
    "    Argumento: un texto con una tupla (con el token y su entidad) para cada elemento en cada frase.\n",
    "    \"\"\"\n",
    "        \n",
    "    entidades_con_posiciones = set()\n",
    "\n",
    "    for sentence_index, sentence in enumerate(train_data_bioes):\n",
    "        ent = []\n",
    "        name = None\n",
    "        start_pos = None  # Posición de inicio de la entidad actual\n",
    "\n",
    "        for token_index, (word, bioes_tag) in enumerate(sentence):\n",
    "            if bioes_tag != 'O':\n",
    "                if bioes_tag.startswith('B-') or bioes_tag.startswith('S-'):\n",
    "                    # Si hay una entidad anterior, la agregamos a la lista de entidades\n",
    "                    if ent:\n",
    "                        end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                        entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                    # Creamos una nueva entidad con la palabra actual\n",
    "                    ent = [word]\n",
    "                    # Obtenemos el tipo de entidad\n",
    "                    name = bioes_tag.split('-')[1]\n",
    "                    start_pos = token_index  # La posición de inicio es el token actual\n",
    "                elif bioes_tag.startswith('I-') or bioes_tag.startswith('E-'):\n",
    "                    ent.append(word)\n",
    "            elif ent:\n",
    "                # Si encontramos una etiqueta 'O' y hay una entidad en curso, la agregamos a la lista de entidades\n",
    "                end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Reiniciamos la lista de la entidad actual\n",
    "                ent = []\n",
    "\n",
    "        # Agregamos la última entidad si la hay\n",
    "        if ent:\n",
    "            end_pos = len(sentence) - 1  # La posición de fin es el último token de la oración\n",
    "            entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "\n",
    "    return entidades_con_posiciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Features per defecte de CRFTagger\n",
      "Precisió: 0.6887340301974448\n",
      "Recall: 0.6565181289786881\n",
      "F1-score: 0.6722403287515941\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor()\n",
      "Precisió: 0.7877838684416602\n",
      "Recall: 0.2784389703847218\n",
      "F1-score: 0.4114519427402863\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True)\n",
      "Precisió: 0.6522241478913923\n",
      "Recall: 0.6249654027124274\n",
      "F1-score: 0.638303886925795\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True)\n",
      "Precisió: 0.7036930178880554\n",
      "Recall: 0.6750622751176307\n",
      "F1-score: 0.6890803785845458\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True, use_context_features=True)\n",
      "Precisió: 0.7442737025224703\n",
      "Recall: 0.7104898975920287\n",
      "F1-score: 0.7269895213820448\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_esp_bioes = convert_to_bioes(train_esp)\n",
    "train_esp_BIOES = obtener_token_entity(train_esp_bioes)\n",
    "\n",
    "testa_esp_bioes = convert_to_bioes(testa_esp)\n",
    "testa_esp_real = obtener_token_entity(testa_esp_bioes)\n",
    "\n",
    "def model_entrenament(train_tag, extractor, testa_pre_tag):\n",
    "    if extractor == None:\n",
    "        model_BIOES = CRFTagger()\n",
    "        model_BIOES.train(train_tag, 'model_BIOES_esp.crf.tagger')\n",
    "        \n",
    "        predicted_BIOES = model_BIOES.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    else:\n",
    "        model_BIOES = CRFTagger(feature_func=extractor._get_features)\n",
    "        model_BIOES.train(train_tag, 'model_BIOES_esp.crf.tagger')\n",
    "        \n",
    "        predicted_BIOES = model_BIOES.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    return resultats(predicted_BIOES, testa_esp_real, obtener_entidades_con_posiciones_bioes)\n",
    "\n",
    "for param in param_combinations_esp: # definida primerament en la predicció del BIO\n",
    "    print(f'Modelo: {str(param)}') if param != None else print(f'Modelo: Features per defecte de CRFTagger')\n",
    "    model_entrenament(train_esp_BIOES, param, testa_esp_pre_tag)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7817327065144393\n",
      "Recall: 0.7673038892551087\n",
      "F1-score: 0.7744510978043911\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True, model_POS=model_tagger_POS_esp)\n",
    "\n",
    "testb_esp_bioes = convert_to_bioes(testb_esp)\n",
    "testb_esp_real = obtener_token_entity(testb_esp_bioes)\n",
    "\n",
    "testb_esp_pre_tag = obtener_token(testb_esp)\n",
    "\n",
    "model_BIOES = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIOES.train(train_esp_BIOES, 'model_BIOES_esp.crf.tagger')\n",
    "    \n",
    "predicted_BIOES = model_BIOES.tag_sents(testb_esp_pre_tag)\n",
    "\n",
    "resultats(predicted_BIOES, testb_esp_real, obtener_entidades_con_posiciones_bioes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Features per defecte de CRFTagger\n",
      "Precisió: 0.0\n",
      "Recall: 0.0\n",
      "F1-score: 0.0\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor()\n",
      "Precisió: 0.7785087719298246\n",
      "Recall: 0.16202647193062528\n",
      "F1-score: 0.26822818284850775\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True)\n",
      "Precisió: 0.6494252873563219\n",
      "Recall: 0.41259698767685987\n",
      "F1-score: 0.504605079542283\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True)\n",
      "Precisió: 0.6781725888324873\n",
      "Recall: 0.6097672295755363\n",
      "F1-score: 0.6421533285267964\n",
      "--------------------------------------------------\n",
      "Modelo: FeatureExtractor(use_basic_features=True, use_prefix_suffix_features=True, use_context_features=True)\n",
      "Precisió: 0.7135802469135802\n",
      "Recall: 0.659516202647193\n",
      "F1-score: 0.6854838709677419\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_ned_bioes = convert_to_bioes(train_ned)\n",
    "train_ned_BIOES = obtener_token_entity(train_ned_bioes)\n",
    "\n",
    "testa_ned_bioes = convert_to_bioes(testa_ned)\n",
    "testa_ned_real = obtener_token_entity(testa_ned_bioes)\n",
    "\n",
    "def model_entrenament(train_tag, extractor, testa_pre_tag):\n",
    "    if extractor == None:\n",
    "        model_BIOES = CRFTagger()\n",
    "        model_BIOES.train(train_tag, 'model_BIOES_ned.crf.tagger')\n",
    "        \n",
    "        predicted_BIOES = model_IO.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    else:\n",
    "        model_BIOES = CRFTagger(feature_func=extractor._get_features)\n",
    "        model_BIOES.train(train_tag, 'model_BIOES_ned.crf.tagger')\n",
    "        \n",
    "        predicted_BIOES = model_BIOES.tag_sents(testa_pre_tag)\n",
    "        \n",
    "    return resultats(predicted_BIOES, testa_ned_real, obtener_entidades_con_posiciones_bioes)\n",
    "\n",
    "for param in param_combinations_ned: # definida primerament en la predicció del BIO\n",
    "    print(f'Modelo: {str(param)}') if param != None else print(f'Modelo: Features per defecte de CRFTagger')\n",
    "    model_entrenament(train_ned_BIOES, param, testa_ned_pre_tag)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7451505016722408\n",
      "Recall: 0.6853275915103045\n",
      "F1-score: 0.7139881429258131\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True, model_POS=model_tagger_POS_ned)\n",
    "\n",
    "testb_ned_bioes = convert_to_bioes(testb_ned)\n",
    "testb_ned_real = obtener_token_entity(testb_ned_bioes)\n",
    "\n",
    "testb_ned_pre_tag = obtener_token(testb_ned)\n",
    "\n",
    "model_BIOES = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIOES.train(train_ned_BIOES, 'model_BIOES_ned.crf.tagger')\n",
    "    \n",
    "predicted_BIOES = model_BIOES.tag_sents(testb_ned_pre_tag)\n",
    "\n",
    "resultats(predicted_BIOES, testb_ned_real, obtener_entidades_con_posiciones_bioes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue; font-weight:bold;\">CONCLUSIONS</span> <a id=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicció amb BIOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_biow(train_data_bio):\n",
    "    train_data_biow = []\n",
    "    for sentence in train_data_bio:\n",
    "        biow_tags = []\n",
    "        for word, pos_tag, bio_tag in sentence:\n",
    "            if bio_tag == 'O':\n",
    "                biow_tags.append('O')\n",
    "            else:\n",
    "                biow_tags.append(bio_tag + 'W')  # Añadir 'W' a todas las etiquetas\n",
    "            \n",
    "        train_data_biow.append(list(zip([word for word, pos_tag, bio_tag in sentence], biow_tags)))\n",
    "    return train_data_biow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_entidades_con_posiciones_biow(train_data_biow):\n",
    "    entidades_con_posiciones = set()\n",
    "\n",
    "    for sentence_index, sentence in enumerate(train_data_biow):\n",
    "        ent = []\n",
    "        name = None\n",
    "        start_pos = None  # Posición de inicio de la entidad actual\n",
    "        prev_tag = None  # Almacenar la etiqueta del token anterior\n",
    "\n",
    "        for token_index, (word, biow_tag) in enumerate(sentence):\n",
    "            if biow_tag != 'O':\n",
    "                if biow_tag.endswith('W'):\n",
    "                    # Eliminar 'W' del final de la etiqueta\n",
    "                    io_tag = biow_tag[:-1]\n",
    "                else:\n",
    "                    io_tag = biow_tag\n",
    "                if io_tag.startswith('I-'):\n",
    "                    ent.append(word)\n",
    "                else:\n",
    "                    # Si hay una entidad anterior, la agregamos a la lista de entidades\n",
    "                    if ent:\n",
    "                        end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                        entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                    # Creamos una nueva entidad con la palabra actual\n",
    "                    ent = [word]\n",
    "                    # Obtenemos el tipo de entidad\n",
    "                    name = io_tag.split('-')[1]\n",
    "                    start_pos = token_index  # La posición de inicio es el token actual\n",
    "                prev_tag = io_tag  # Actualizamos la etiqueta del token anterior\n",
    "            elif ent:\n",
    "                # Si encontramos una etiqueta 'O' y hay una entidad en curso, la agregamos a la lista de entidades\n",
    "                end_pos = token_index - 1  # La posición de fin es el token anterior\n",
    "                entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "                # Reiniciamos la lista de la entidad actual\n",
    "                ent = []\n",
    "                prev_tag = None  # Reiniciamos la etiqueta del token anterior\n",
    "\n",
    "        # Agregamos la última entidad si la hay\n",
    "        if ent:\n",
    "            end_pos = len(sentence) - 1  # La posición de fin es el último token de la oración\n",
    "            entidades_con_posiciones.add((tuple(ent), (start_pos, end_pos), name))\n",
    "\n",
    "    return entidades_con_posiciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_precision(entidades_referencia, entidades_extraidas):\n",
    "    # Calcular el número de entidades correctamente extraídas\n",
    "    entidades_correctas = entidades_referencia.intersection(entidades_extraidas)\n",
    "    \n",
    "    # Calcular la precisión\n",
    "    if len(entidades_extraidas) > 0:\n",
    "        precision = len(entidades_correctas) / len(entidades_extraidas)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    return precision\n",
    "\n",
    "\n",
    "def calcular_exhaustividad(entidades_referencia, entidades_extraidas):\n",
    "    # Calcular el número de entidades correctamente extraídas\n",
    "    entidades_correctas = entidades_referencia.intersection(entidades_extraidas)\n",
    "    \n",
    "    # Calcular la exhaustividad\n",
    "    if len(entidades_referencia) > 0:\n",
    "        exhaustividad = len(entidades_correctas) / len(entidades_referencia)\n",
    "    else:\n",
    "        exhaustividad = 0.0\n",
    "    \n",
    "    return exhaustividad\n",
    "\n",
    "def calcular_f1_score(precision, exhaustividad):\n",
    "    # Calcular el F1-score\n",
    "    if (precision + exhaustividad) > 0:\n",
    "        f1_score = 2 * (precision * exhaustividad) / (precision + exhaustividad)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "def resultats(predicted_BIO, testa_esp_IO_tag):\n",
    "\n",
    "    # Obtener los conjuntos de entidades de referencia y extraídas\n",
    "    entidades_referencia = obtener_entidades_con_posiciones_biow(testa_esp_IO_tag)  # Conjunto de entidades etiquetadas manualmente como referencia\n",
    "    entidades_extraidas =  obtener_entidades_con_posiciones_biow(predicted_BIO) # Obtener conjuntos de entidades extraídas\n",
    "\n",
    "    # Calcular la precisión\n",
    "    precision = calcular_precision(entidades_referencia, entidades_extraidas)\n",
    "\n",
    "    # Calcular la exhaustividad\n",
    "    exhaustividad = calcular_exhaustividad(entidades_referencia, entidades_extraidas)\n",
    "\n",
    "    # Calcular el F1-score\n",
    "    f1_score = calcular_f1_score(precision, exhaustividad)\n",
    "\n",
    "    print(\"Precisió:\", precision)\n",
    "    print(\"Exhaustivitat:\", exhaustividad)\n",
    "    print(\"F1-score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  1\n",
      "Precisió: 0.0\n",
      "Exhaustivitat: 0.0\n",
      "F1-score: 0.0\n",
      "--------------------------------------------------\n",
      "Model:  2\n",
      "Precisió: 0.39166666666666666\n",
      "Exhaustivitat: 0.36424024356490453\n",
      "F1-score: 0.37745590133371576\n",
      "--------------------------------------------------\n",
      "Model:  3\n",
      "Precisió: 0.6693500298151461\n",
      "Exhaustivitat: 0.6213672848048712\n",
      "F1-score: 0.6444667719247883\n",
      "--------------------------------------------------\n",
      "Model:  4\n",
      "Precisió: 0.7517543859649123\n",
      "Exhaustivitat: 0.7115970107943537\n",
      "F1-score: 0.7311246978529788\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_combinations = [\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True),\n",
    "    FeatureExtractor(True, True),\n",
    "    FeatureExtractor(True, True, True)\n",
    "]\n",
    "\n",
    "testa_esp_pre_tag = obtener_token(testa_esp)\n",
    "\n",
    "train_esp_BIOW = convert_to_biow(train_esp)\n",
    "\n",
    "testa_esp_real = convert_to_biow(testa_esp)\n",
    "\n",
    "def model_entrenament(train, extractor, testa):\n",
    "    \n",
    "    model_BIOW = CRFTagger(feature_func=extractor._get_features)\n",
    "    model_BIOW.train(train, 'model_BIOW.crf.tagger')\n",
    "    \n",
    "    predicted_BIOW = model_BIOW.tag_sents(testa)\n",
    "    \n",
    "    return resultats(predicted_BIOW, testa_esp_real)\n",
    "i = 1\n",
    "for param in param_combinations:\n",
    "    print('Model: ', i)\n",
    "    model_entrenament(train_esp_BIOW, param, testa_esp_pre_tag)\n",
    "    print('-'*50)\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7885268160217244\n",
      "Exhaustivitat: 0.7656558998022412\n",
      "F1-score: 0.7769230769230768\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True)\n",
    "\n",
    "testa_esp_real = convert_to_biow(testb_esp)\n",
    "testa_esp_pre_tag = obtener_token(testb_esp)\n",
    "\n",
    "model_BIOW = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIOW.train(train_esp_BIOW, 'model_BIOW.crf.tagger')\n",
    "    \n",
    "predicted_BIOW = model_BIOW.tag_sents(testa_esp_pre_tag)\n",
    "\n",
    "resultats(predicted_BIOW, testa_esp_real)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  1\n",
      "Precisió: 0.5769230769230769\n",
      "Exhaustivitat: 0.013692377909630305\n",
      "F1-score: 0.026749888542131076\n",
      "--------------------------------------------------\n",
      "Model:  2\n",
      "Precisió: 0.2477326968973747\n",
      "Exhaustivitat: 0.2368781378366043\n",
      "F1-score: 0.24218385440970602\n",
      "--------------------------------------------------\n",
      "Model:  3\n",
      "Precisió: 0.6316606311433006\n",
      "Exhaustivitat: 0.5572797809219534\n",
      "F1-score: 0.5921435499515034\n",
      "--------------------------------------------------\n",
      "Model:  4\n",
      "Precisió: 0.6983735830458354\n",
      "Exhaustivitat: 0.6467366499315381\n",
      "F1-score: 0.6715639810426541\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_combinations = [\n",
    "    FeatureExtractor(),\n",
    "    FeatureExtractor(True),\n",
    "    FeatureExtractor(True, True),\n",
    "    FeatureExtractor(True, True, True)\n",
    "]\n",
    "\n",
    "testa_ned_pre_tag = obtener_token(testa_ned)\n",
    "\n",
    "train_ned_BIOW = convert_to_biow(train_ned)\n",
    "\n",
    "testa_ned_real = convert_to_biow(testa_ned)\n",
    "\n",
    "def model_entrenament(train, extractor, testa):\n",
    "    \n",
    "    model_BIOW = CRFTagger(feature_func=extractor._get_features)\n",
    "    model_BIOW.train(train, 'model_BIOW.crf.tagger')\n",
    "    \n",
    "    predicted_BIOW = model_BIOW.tag_sents(testa)\n",
    "    \n",
    "    return resultats(predicted_BIOW, testa_ned_real)\n",
    "i = 1\n",
    "for param in param_combinations:\n",
    "    print('Model: ', i)\n",
    "    model_entrenament(train_ned_BIOW, param, testa_ned_pre_tag)\n",
    "    print('-'*50)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisió: 0.7248165443629087\n",
      "Exhaustivitat: 0.6684097200861273\n",
      "F1-score: 0.6954712754040647\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(True, True, True)\n",
    "\n",
    "testa_ned_real = convert_to_biow(testb_ned)\n",
    "testa_ned_pre_tag = obtener_token(testb_ned)\n",
    "\n",
    "model_BIOW = CRFTagger(feature_func=feature_extractor._get_features)\n",
    "model_BIOW.train(train_ned_BIOW, 'model_BIOW.crf.tagger')\n",
    "    \n",
    "predicted_BIOW = model_BIOW.tag_sents(testa_ned_pre_tag)\n",
    "\n",
    "resultats(predicted_BIOW, testa_ned_real)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
